<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Wed, 29 May 2024 16:23:36 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Enhance image search experiences with Amazon Personalize, Amazon OpenSearch Service, and Amazon Titan Multimodal Embeddings in Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/29_05_2024/29_05_2024_4/</link>
      <pubDate>Wed, 29 May 2024 16:23:36 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_05_2024/29_05_2024_4/</guid>
      <description>
        
          
            A variety of different techniques have been used for returning images relevant to search queries. Historically, the idea of creating a joint embedding space to facilitate image captioning or text-to-image search has been of interest to machine learning (ML) practitioners and businesses for quite a while. Contrastive Language–Image Pre-training (CLIP) and Bootstrapping Language-Image Pre-training (BLIP) […]
Link to article: https://aws.amazon.com/blogs/machine-learning/enhance-image-search-experiences-with-amazon-personalize-amazon-opensearch-service-and-amazon-titan-multimodal-embeddings-in-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>End-to-end LLM training on instance clusters with over 100 nodes using AWS Trainium</title>
      <link>https://www.dotnetramblings.com/post/29_05_2024/29_05_2024_5/</link>
      <pubDate>Wed, 29 May 2024 16:19:09 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_05_2024/29_05_2024_5/</guid>
      <description>
        
          
            In this post, we show you how to accelerate the full pre-training of LLM models by scaling up to 128 trn1.32xlarge nodes, using a Llama 2-7B model as an example. We share best practices for training LLMs on AWS Trainium, scaling the training on a cluster with over 100 nodes, improving efficiency of recovery from system and hardware failures, improving training stability, and achieving convergence.
Link to article: https://aws.amazon.com/blogs/machine-learning/end-to-end-llm-training-on-instance-clusters-with-over-100-nodes-using-aws-trainium/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Fine-tune large multimodal models using Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/29_05_2024/29_05_2024_6/</link>
      <pubDate>Wed, 29 May 2024 16:00:38 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_05_2024/29_05_2024_6/</guid>
      <description>
        
          
            Large multimodal models (LMMs) integrate multiple data types into a single model. By combining text data with images and other modalities during training, multimodal models such as Claude3, GPT-4V, and Gemini Pro Vision gain more comprehensive understanding and improved ability to process diverse data types. The multimodal approach allows models to handle a wider range […]
Link to article: https://aws.amazon.com/blogs/machine-learning/fine-tune-large-multimodal-models-using-amazon-sagemaker/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Accelerate Mixtral 8x7B pre-training with expert parallelism on Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/23_05_2024/23_05_2024_1/</link>
      <pubDate>Thu, 23 May 2024 19:08:31 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/23_05_2024/23_05_2024_1/</guid>
      <description>
        
          
            Mixture of Experts (MoE) architectures for large language models (LLMs) have recently gained popularity due to their ability to increase model capacity and computational efficiency compared to fully dense models. By utilizing sparse expert subnetworks that process different subsets of tokens, MoE models can effectively increase the number of parameters while requiring less computation per […]
Link to article: https://aws.amazon.com/blogs/machine-learning/accelerate-mixtral-8x7b-pre-training-with-expert-parallelism-on-amazon-sagemaker/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
