<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Thu, 01 May 2025 21:08:27 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Best practices for Meta Llama 3.2 multimodal fine-tuning on Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_0/</link>
      <pubDate>Thu, 01 May 2025 21:08:27 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_0/</guid>
      <description>
        
          
            In this post, we share comprehensive best practices and scientific insights for fine-tuning Meta Llama 3.2 multimodal models on Amazon Bedrock. By following these guidelines, you can fine-tune smaller, more cost-effective models to achieve performance that rivals or even surpasses much larger models—potentially reducing both inference costs and latency, while maintaining high accuracy for your specific use case.
Link to article: https://aws.amazon.com/blogs/machine-learning/best-practices-for-meta-llama-3-2-multimodal-fine-tuning-on-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Extend large language models powered by Amazon SageMaker AI using Model Context Protocol</title>
      <link>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_2/</link>
      <pubDate>Thu, 01 May 2025 17:21:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_2/</guid>
      <description>
        
          
            The MCP proposed by Anthropic offers a standardized way of connecting FMs to data sources, and now you can use this capability with SageMaker AI. In this post, we presented an example of combining the power of SageMaker AI and MCP to build an application that offers a new perspective on loan underwriting through specialized roles and automated workflows.
Link to article: https://aws.amazon.com/blogs/machine-learning/extend-large-language-models-powered-by-amazon-sagemaker-ai-using-model-context-protocol/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Automate document translation and standardization with Amazon Bedrock and Amazon Translate</title>
      <link>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_5/</link>
      <pubDate>Thu, 01 May 2025 16:54:33 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_5/</guid>
      <description>
        
          
            In this post, we show how you can automate language localization through translating documents using Amazon Web Services (AWS). The solution combines Amazon Bedrock and AWS Serverless technologies, a suite of fully managed event-driven services for running code, managing data, and integrating applications—all without managing servers.
Link to article: https://aws.amazon.com/blogs/machine-learning/automate-document-translation-and-standardization-with-amazon-bedrock-and-amazon-translate/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Autonomous mortgage processing using Amazon Bedrock Data Automation and Amazon Bedrock Agents</title>
      <link>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_6/</link>
      <pubDate>Thu, 01 May 2025 16:52:27 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_6/</guid>
      <description>
        
          
            In this post, we introduce agentic automatic mortgage approval, a next-generation sample solution that uses autonomous AI agents powered by Amazon Bedrock Agents and Amazon Bedrock Data Automation. These agents orchestrate the entire mortgage approval process—intelligently verifying documents, assessing risk, and making data-driven decisions with minimal human intervention.
Link to article: https://aws.amazon.com/blogs/machine-learning/autonomous-mortgage-processing-using-amazon-bedrock-data-automation-and-amazon-bedrock-agents/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Amazon Bedrock Model Distillation: Boost function calling accuracy while reducing cost and latency</title>
      <link>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_18/</link>
      <pubDate>Thu, 01 May 2025 01:04:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/01_05_2025/01_05_2025_18/</guid>
      <description>
        
          
            In this post, we highlight the advanced data augmentation techniques and performance improvements in Amazon Bedrock Model Distillation with Meta&#39;s Llama model family. This technique transfers knowledge from larger, more capable foundation models (FMs) that act as teachers to smaller, more efficient models (students), creating specialized models that excel at specific tasks.
Link to article: https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-model-distillation-boost-function-calling-accuracy-while-reducing-cost-and-latency/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build public-facing generative AI applications using Amazon Q Business for anonymous users</title>
      <link>https://www.dotnetramblings.com/post/30_04_2025/30_04_2025_1/</link>
      <pubDate>Wed, 30 Apr 2025 19:00:10 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/30_04_2025/30_04_2025_1/</guid>
      <description>
        
          
            Today, we’re excited to announce that Amazon Q Business now supports anonymous user access. With this new feature, you can now create Amazon Q Business applications with anonymous user mode, where user authentication is not required and content is publicly accessible. In this post, we demonstrate how to build a public-facing generative AI application using Amazon Q Business for anonymous users.
Link to article: https://aws.amazon.com/blogs/machine-learning/build-public-facing-generative-ai-applications-using-amazon-q-business-for-anonymous-users/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>FloQast builds an AI-powered accounting transformation solution with Anthropic’s Claude 3 on Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/30_04_2025/30_04_2025_2/</link>
      <pubDate>Wed, 30 Apr 2025 18:37:32 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/30_04_2025/30_04_2025_2/</guid>
      <description>
        
          
            In this post, we share how FloQast built an AI-powered accounting transaction solution using Anthropic’s Claude 3 on Amazon Bedrock.
Link to article: https://aws.amazon.com/blogs/machine-learning/floqast-builds-an-ai-powered-accounting-transformation-solution-with-anthropics-claude-3-on-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Insights in implementing production-ready solutions with generative AI</title>
      <link>https://www.dotnetramblings.com/post/30_04_2025/30_04_2025_3/</link>
      <pubDate>Wed, 30 Apr 2025 18:34:40 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/30_04_2025/30_04_2025_3/</guid>
      <description>
        
          
            As generative AI revolutionizes industries, organizations are eager to harness its potential. However, the journey from production-ready solutions to full-scale implementation can present distinct operational and technical considerations. This post explores key insights and lessons learned from AWS customers in Europe, Middle East, and Africa (EMEA) who have successfully navigated this transition, providing a roadmap for others looking to follow suit.
Link to article: https://aws.amazon.com/blogs/machine-learning/insights-in-implementing-production-ready-solutions-with-generative-ai/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Responsible AI in action: How Data Reply red teaming supports generative AI safety on AWS</title>
      <link>https://www.dotnetramblings.com/post/29_04_2025/29_04_2025_3/</link>
      <pubDate>Tue, 29 Apr 2025 16:32:18 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_04_2025/29_04_2025_3/</guid>
      <description>
        
          
            In this post, we explore how AWS services can be seamlessly integrated with open source tools to help establish a robust red teaming mechanism within your organization. Specifically, we discuss Data Reply’s red teaming solution, a comprehensive blueprint to enhance AI safety and responsible AI practices.
Link to article: https://aws.amazon.com/blogs/machine-learning/responsible-ai-in-action-how-data-reply-red-teaming-supports-generative-ai-safety-on-aws/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>InterVision accelerates AI development using AWS LLM League and Amazon SageMaker AI</title>
      <link>https://www.dotnetramblings.com/post/29_04_2025/29_04_2025_4/</link>
      <pubDate>Tue, 29 Apr 2025 16:21:46 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_04_2025/29_04_2025_4/</guid>
      <description>
        
          
            This post demonstrates how AWS LLM League’s gamified enablement accelerates partners’ practical AI development capabilities, while showcasing how fine-tuning smaller language models can deliver cost-effective, specialized solutions for specific industry needs.
Link to article: https://aws.amazon.com/blogs/machine-learning/intervision-accelerates-ai-development-using-aws-llm-league-and-amazon-sagemaker-ai/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Improve Amazon Nova migration performance with data-aware prompt optimization</title>
      <link>https://www.dotnetramblings.com/post/29_04_2025/29_04_2025_5/</link>
      <pubDate>Tue, 29 Apr 2025 16:18:24 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_04_2025/29_04_2025_5/</guid>
      <description>
        
          
            In this post, we present an LLM migration paradigm and architecture, including a continuous process of model evaluation, prompt generation using Amazon Bedrock, and data-aware optimization. The solution evaluates the model performance before migration and iteratively optimizes the Amazon Nova model prompts using user-provided dataset and objective metrics.
Link to article: https://aws.amazon.com/blogs/machine-learning/improve-amazon-nova-migration-performance-with-data-aware-prompt-optimization/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Customize Amazon Nova models to improve tool usage</title>
      <link>https://www.dotnetramblings.com/post/28_04_2025/28_04_2025_2/</link>
      <pubDate>Mon, 28 Apr 2025 17:47:59 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/28_04_2025/28_04_2025_2/</guid>
      <description>
        
          
            In this post, we demonstrate model customization (fine-tuning) for tool use with Amazon Nova. We first introduce a tool usage use case, and gave details about the dataset. We walk through the details of Amazon Nova specific data formatting and showed how to do tool calling through the Converse and Invoke APIs in Amazon Bedrock. After getting the baseline results from Amazon Nova models, we explain in detail the fine-tuning process, hosting fine-tuned models with provisioned throughput, and using the fine-tuned Amazon Nova models for inference.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Evaluate Amazon Bedrock Agents with Ragas and LLM-as-a-judge</title>
      <link>https://www.dotnetramblings.com/post/28_04_2025/28_04_2025_4/</link>
      <pubDate>Mon, 28 Apr 2025 15:31:56 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/28_04_2025/28_04_2025_4/</guid>
      <description>
        
          
            In this post, we introduced the Open Source Bedrock Agent Evaluation framework, a Langfuse-integrated solution that streamlines the agent development process. We demonstrated how this evaluation framework can be integrated with pharmaceutical research agents. We used it to evaluate agent performance against biomarker questions and sent traces to Langfuse to view evaluation metrics across question types.
Link to article: https://aws.amazon.com/blogs/machine-learning/evaluate-amazon-bedrock-agents-with-ragas-and-llm-as-a-judge/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Enterprise-grade natural language to SQL generation using LLMs: Balancing accuracy, latency, and scale</title>
      <link>https://www.dotnetramblings.com/post/24_04_2025/24_04_2025_1/</link>
      <pubDate>Thu, 24 Apr 2025 16:23:48 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_04_2025/24_04_2025_1/</guid>
      <description>
        
          
            In this post, the AWS and Cisco teams unveil a new methodical approach that addresses the challenges of enterprise-grade SQL generation. The teams were able to reduce the complexity of the NL2SQL process while delivering higher accuracy and better overall performance.
Link to article: https://aws.amazon.com/blogs/machine-learning/enterprise-grade-natural-language-to-sql-generation-using-llms-balancing-accuracy-latency-and-scale/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>AWS Field Experience reduced cost and delivered low latency and high performance with Amazon Nova Lite foundation model</title>
      <link>https://www.dotnetramblings.com/post/24_04_2025/24_04_2025_2/</link>
      <pubDate>Thu, 24 Apr 2025 16:17:50 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_04_2025/24_04_2025_2/</guid>
      <description>
        
          
            The AFX team’s product migration to the Nova Lite model has delivered tangible enterprise value by enhancing sales workflows. By migrating to the Amazon Nova Lite model, the team has not only achieved significant cost savings and reduced latency, but has also empowered sellers with a leading intelligent and reliable solution.
Link to article: https://aws.amazon.com/blogs/machine-learning/aws-field-experience-reduced-cost-and-delivered-low-latency-and-high-performance-with-amazon-nova-lite-foundation-model/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Combine keyword and semantic search for text and images using Amazon Bedrock and Amazon OpenSearch Service</title>
      <link>https://www.dotnetramblings.com/post/24_04_2025/24_04_2025_3/</link>
      <pubDate>Thu, 24 Apr 2025 16:13:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_04_2025/24_04_2025_3/</guid>
      <description>
        
          
            In this post, we walk you through how to build a hybrid search solution using OpenSearch Service powered by multimodal embeddings from the Amazon Titan Multimodal Embeddings G1 model through Amazon Bedrock. This solution demonstrates how you can enable users to submit both text and images as queries to retrieve relevant results from a sample retail image dataset.
Link to article: https://aws.amazon.com/blogs/machine-learning/combine-keyword-and-semantic-search-for-text-and-images-using-amazon-bedrock-and-amazon-opensearch-service/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
