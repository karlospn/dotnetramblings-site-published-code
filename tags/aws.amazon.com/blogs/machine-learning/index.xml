<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Thu, 24 Jul 2025 20:17:18 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Boost cold-start recommendations with vLLM on AWS Trainium</title>
      <link>https://www.dotnetramblings.com/post/24_07_2025/24_07_2025_1/</link>
      <pubDate>Thu, 24 Jul 2025 20:17:18 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_07_2025/24_07_2025_1/</guid>
      <description>
        
          
            In this post, we demonstrate how to use vLLM for scalable inference and use AWS Deep Learning Containers (DLC) to streamline model packaging and deployment. We’ll generate interest expansions through structured prompts, encode them into embeddings, retrieve candidates with FAISS, apply validation to keep results grounded, and frame the cold-start challenge as a scientific experiment—benchmarking LLM and encoder pairings, iterating rapidly on recommendation metrics, and showing clear ROI for each configuration
          
          
        
      </description>
    </item>
    
    <item>
      <title>Benchmarking Amazon Nova: A comprehensive analysis through MT-Bench and Arena-Hard-Auto</title>
      <link>https://www.dotnetramblings.com/post/24_07_2025/24_07_2025_2/</link>
      <pubDate>Thu, 24 Jul 2025 18:39:08 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_07_2025/24_07_2025_2/</guid>
      <description>
        
          
            The repositories for MT-Bench and Arena-Hard were originally developed using OpenAI’s GPT API, primarily employing GPT-4 as the judge. Our team has expanded its functionality by integrating it with the Amazon Bedrock API to enable using Anthropic’s Claude Sonnet on Amazon as judge. In this post, we use both MT-Bench and Arena-Hard to benchmark Amazon Nova models by comparing them to other leading LLMs available through Amazon Bedrock.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Customize Amazon Nova in Amazon SageMaker AI using Direct Preference Optimization</title>
      <link>https://www.dotnetramblings.com/post/23_07_2025/23_07_2025_2/</link>
      <pubDate>Wed, 23 Jul 2025 19:08:16 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/23_07_2025/23_07_2025_2/</guid>
      <description>
        
          
            At the AWS Summit in New York City, we introduced a comprehensive suite of model customization capabilities for Amazon Nova foundation models. Available as ready-to-use recipes on Amazon SageMaker AI, you can use them to adapt Nova Micro, Nova Lite, and Nova Pro across the model training lifecycle, including pre-training, supervised fine-tuning, and alignment. In this post, we present a streamlined approach to customize Nova Micro in SageMaker training jobs.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Multi-tenant RAG implementation with Amazon Bedrock and Amazon OpenSearch Service for SaaS using JWT</title>
      <link>https://www.dotnetramblings.com/post/23_07_2025/23_07_2025_4/</link>
      <pubDate>Wed, 23 Jul 2025 16:44:31 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/23_07_2025/23_07_2025_4/</guid>
      <description>
        
          
            In this post, we introduce a solution that uses OpenSearch Service as a vector data store in multi-tenant RAG, achieving data isolation and routing using JWT and FGAC. This solution uses a combination of JWT and FGAC to implement strict tenant data access isolation and routing, necessitating the use of OpenSearch Service.
Link to article: https://aws.amazon.com/blogs/machine-learning/multi-tenant-rag-implementation-with-amazon-bedrock-and-amazon-opensearch-service-for-saas-using-jwt/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Enhance generative AI solutions using Amazon Q index with Model Context Protocol – Part 1</title>
      <link>https://www.dotnetramblings.com/post/23_07_2025/23_07_2025_5/</link>
      <pubDate>Wed, 23 Jul 2025 16:40:40 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/23_07_2025/23_07_2025_5/</guid>
      <description>
        
          
            In this post, we explore best practices and integration patterns for combining Amazon Q index and MCP, enabling enterprises to build secure, scalable, and actionable AI search-and-retrieval architectures.
Link to article: https://aws.amazon.com/blogs/machine-learning/enhance-generative-ai-solutions-using-amazon-q-index-with-model-context-protocol-part-1/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Beyond accelerators: Lessons from building foundation models on AWS with Japan’s GENIAC program</title>
      <link>https://www.dotnetramblings.com/post/22_07_2025/22_07_2025_1/</link>
      <pubDate>Tue, 22 Jul 2025 16:42:36 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_07_2025/22_07_2025_1/</guid>
      <description>
        
          
            In 2024, the Ministry of Economy, Trade and Industry (METI) launched the Generative AI Accelerator Challenge (GENIAC)—a Japanese national program to boost generative AI by providing companies with funding, mentorship, and massive compute resources for foundation model (FM) development. AWS was selected as the cloud provider for GENIAC’s second cycle (cycle 2). It provided infrastructure and technical guidance for 12 participating organizations.
Link to article: https://aws.amazon.com/blogs/machine-learning/beyond-accelerators-lessons-from-building-foundation-models-on-aws-with-japans-geniac-program/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Streamline deep learning environments with Amazon Q Developer and MCP</title>
      <link>https://www.dotnetramblings.com/post/22_07_2025/22_07_2025_3/</link>
      <pubDate>Tue, 22 Jul 2025 14:52:50 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_07_2025/22_07_2025_3/</guid>
      <description>
        
          
            In this post, we explore how to use Amazon Q Developer and Model Context Protocol (MCP) servers to streamline DLC workflows to automate creation, execution, and customization of DLC containers.
Link to article: https://aws.amazon.com/blogs/machine-learning/streamline-deep-learning-environments-with-amazon-q-developer-and-mcp/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build an AI-powered automated summarization system with Amazon Bedrock and Amazon Transcribe using Terraform</title>
      <link>https://www.dotnetramblings.com/post/21_07_2025/21_07_2025_2/</link>
      <pubDate>Mon, 21 Jul 2025 17:34:21 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_07_2025/21_07_2025_2/</guid>
      <description>
        
          
            This post introduces a serverless meeting summarization system that harnesses the advanced capabilities of Amazon Bedrock and Amazon Transcribe to transform audio recordings into concise, structured, and actionable summaries. By automating this process, organizations can reclaim countless hours while making sure key insights, action items, and decisions are systematically captured and made accessible to stakeholders.
Link to article: https://aws.amazon.com/blogs/machine-learning/build-an-ai-powered-automated-summarization-system-with-amazon-bedrock-and-amazon-transcribe-using-terraform/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Kyruus builds a generative AI provider matching solution on AWS</title>
      <link>https://www.dotnetramblings.com/post/21_07_2025/21_07_2025_3/</link>
      <pubDate>Mon, 21 Jul 2025 17:23:36 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_07_2025/21_07_2025_3/</guid>
      <description>
        
          
            In this post, we demonstrate how Kyruus Health uses AWS services to build Guide. We show how Amazon Bedrock, a fully managed service that provides access to foundation models (FMs) from leading AI companies and Amazon through a single API, and Amazon OpenSearch Service, a managed search and analytics service, work together to understand everyday language about health concerns and connect members with the right providers.
Link to article: https://aws.amazon.com/blogs/machine-learning/kyruus-builds-a-generative-ai-provider-matching-solution-on-aws/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Use generative AI in Amazon Bedrock for enhanced recommendation generation in equipment maintenance</title>
      <link>https://www.dotnetramblings.com/post/21_07_2025/21_07_2025_4/</link>
      <pubDate>Mon, 21 Jul 2025 17:10:55 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_07_2025/21_07_2025_4/</guid>
      <description>
        
          
            In the manufacturing world, valuable insights from service reports often remain underutilized in document storage systems. This post explores how Amazon Web Services (AWS) customers can build a solution that automates the digitisation and extraction of crucial information from many reports using generative AI.
Link to article: https://aws.amazon.com/blogs/machine-learning/use-generative-ai-in-amazon-bedrock-for-enhanced-recommendation-generation-in-equipment-maintenance/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build real-time travel recommendations using AI agents on Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/18_07_2025/18_07_2025_0/</link>
      <pubDate>Fri, 18 Jul 2025 16:18:32 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/18_07_2025/18_07_2025_0/</guid>
      <description>
        
          
            In this post, we show how to build a generative AI solution using Amazon Bedrock that creates bespoke holiday packages by combining customer profiles and preferences with real-time pricing data. We demonstrate how to use Amazon Bedrock Knowledge Bases for travel information, Amazon Bedrock Agents for real-time flight details, and Amazon OpenSearch Serverless for efficient package search and retrieval.
Link to article: https://aws.amazon.com/blogs/machine-learning/build-real-time-travel-recommendations-using-ai-agents-on-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Deploy a full stack voice AI agent with Amazon Nova Sonic</title>
      <link>https://www.dotnetramblings.com/post/18_07_2025/18_07_2025_1/</link>
      <pubDate>Fri, 18 Jul 2025 16:14:43 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/18_07_2025/18_07_2025_1/</guid>
      <description>
        
          
            In this post, we show how to create an AI-powered call center agent for a fictional company called AnyTelco. The agent, named Telly, can handle customer inquiries about plans and services while accessing real-time customer data using custom tools implemented with the Model Context Protocol (MCP) framework.
Link to article: https://aws.amazon.com/blogs/machine-learning/deploy-a-full-stack-voice-ai-agent-with-amazon-nova-sonic/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Manage multi-tenant Amazon Bedrock costs using application inference profiles</title>
      <link>https://www.dotnetramblings.com/post/18_07_2025/18_07_2025_2/</link>
      <pubDate>Fri, 18 Jul 2025 16:11:22 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/18_07_2025/18_07_2025_2/</guid>
      <description>
        
          
            This post explores how to implement a robust monitoring solution for multi-tenant AI deployments using a feature of Amazon Bedrock called application inference profiles. We demonstrate how to create a system that enables granular usage tracking, accurate cost allocation, and dynamic resource management across complex multi-tenant environments.
Link to article: https://aws.amazon.com/blogs/machine-learning/manage-multi-tenant-amazon-bedrock-costs-using-application-inference-profiles/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
