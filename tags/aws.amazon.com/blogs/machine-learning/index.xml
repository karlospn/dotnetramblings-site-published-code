<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Wed, 27 Nov 2024 14:57:05 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Create a generative AI assistant with Slack and Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_2/</link>
      <pubDate>Wed, 27 Nov 2024 14:57:05 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_2/</guid>
      <description>
        
          
            Seamless integration of customer experience, collaboration tools, and relevant data is the foundation for delivering knowledge-based productivity gains. In this post, we show you how to integrate the popular Slack messaging service with AWS generative AI services to build a natural language assistant where business users can ask questions of an unstructured dataset.
Link to article: https://aws.amazon.com/blogs/machine-learning/create-a-generative-ai-assistant-with-slack-and-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Unleash your Salesforce data using the Amazon Q Salesforce Online connector</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_0/</link>
      <pubDate>Tue, 26 Nov 2024 23:09:31 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_0/</guid>
      <description>
        
          
            In this post, we walk you through configuring and setting up the Amazon Q Salesforce Online connector. Thousands of companies worldwide use Salesforce to manage their sales, marketing, customer service, and other business operations. The Salesforce cloud-based platform centralizes customer information and interactions across the organization, providing sales reps, marketers, and support agents with a unified 360-degree view of each customer. With Salesforce at the heart of their business, companies accumulate vast amounts of customer data within the platform over time.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Reducing hallucinations in large language models with custom intervention using Amazon Bedrock Agents</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_1/</link>
      <pubDate>Tue, 26 Nov 2024 22:14:48 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_1/</guid>
      <description>
        
          
            This post demonstrates how to use Amazon Bedrock Agents, Amazon Knowledge Bases, and the RAGAS evaluation metrics to build a custom hallucination detector and remediate it by using human-in-the-loop. The agentic workflow can be extended to custom use cases through different hallucination remediation techniques and offers the flexibility to detect and mitigate hallucinations using custom actions.
Link to article: https://aws.amazon.com/blogs/machine-learning/reducing-hallucinations-in-large-language-models-with-custom-intervention-using-amazon-bedrock-agents/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Deploy Meta Llama 3.1-8B on AWS Inferentia using Amazon EKS and vLLM</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_2/</link>
      <pubDate>Tue, 26 Nov 2024 22:12:34 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_2/</guid>
      <description>
        
          
            In this post, we walk through the steps to deploy the Meta Llama 3.1-8B model on Inferentia 2 instances using Amazon EKS. This solution combines the exceptional performance and cost-effectiveness of Inferentia 2 chips with the robust and flexible landscape of Amazon EKS. Inferentia 2 chips deliver high throughput and low latency inference, ideal for LLMs.
Link to article: https://aws.amazon.com/blogs/machine-learning/deploy-meta-llama-3-1-8b-on-aws-inferentia-using-amazon-eks-and-vllm/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Serving LLMs using vLLM and Amazon EC2 instances with AWS AI chips</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_3/</link>
      <pubDate>Tue, 26 Nov 2024 22:07:52 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_3/</guid>
      <description>
        
          
            The use of large language models (LLMs) and generative AI has exploded over the last year. With the release of powerful publicly available foundation models, tools for training, fine tuning and hosting your own LLM have also become democratized. Using vLLM on AWS Trainium and Inferentia makes it possible to host LLMs for high performance […]
Link to article: https://aws.amazon.com/blogs/machine-learning/serving-llms-using-vllm-and-amazon-ec2-instances-with-aws-ai-chips/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Using LLMs to fortify cyber defenses: Sophos’s insight on strategies for using LLMs with Amazon Bedrock and Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_5/</link>
      <pubDate>Tue, 26 Nov 2024 18:52:46 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_5/</guid>
      <description>
        
          
            In this post, SophosAI shares insights in using and evaluating an out-of-the-box LLM for the enhancement of a security operations center’s (SOC) productivity using Amazon Bedrock and Amazon SageMaker. We use Anthropic’s Claude 3 Sonnet on Amazon Bedrock to illustrate the use cases.
Link to article: https://aws.amazon.com/blogs/machine-learning/using-llms-to-fortify-cyber-defenses-sophoss-insight-on-strategies-for-using-llms-with-amazon-bedrock-and-amazon-sagemaker/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Enhanced observability for AWS Trainium and AWS Inferentia with Datadog</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_7/</link>
      <pubDate>Tue, 26 Nov 2024 17:53:11 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_7/</guid>
      <description>
        
          
            This post walks you through Datadog’s new integration with AWS Neuron, which helps you monitor your AWS Trainium and AWS Inferentia instances by providing deep observability into resource utilization, model execution performance, latency, and real-time infrastructure health, enabling you to optimize machine learning (ML) workloads and achieve high-performance at scale.
Link to article: https://aws.amazon.com/blogs/machine-learning/enhanced-observability-for-aws-trainium-and-aws-inferentia-with-datadog/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Create a virtual stock technical analyst using Amazon Bedrock Agents</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_8/</link>
      <pubDate>Tue, 26 Nov 2024 17:19:09 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_8/</guid>
      <description>
        
          
            n this post, we create a virtual analyst that can answer natural language queries of stocks matching certain technical indicator criteria using Amazon Bedrock Agents.
Link to article: https://aws.amazon.com/blogs/machine-learning/create-a-virtual-stock-technical-analyst-using-amazon-bedrock-agents/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Apply Amazon SageMaker Studio lifecycle configurations using AWS CDK</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_9/</link>
      <pubDate>Tue, 26 Nov 2024 17:15:27 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_9/</guid>
      <description>
        
          
            This post serves as a step-by-step guide on how to set up lifecycle configurations for your Amazon SageMaker Studio domains. With lifecycle configurations, system administrators can apply automated controls to their SageMaker Studio domains and their users. We cover core concepts of SageMaker Studio and provide code examples of how to apply lifecycle configuration to […]
Link to article: https://aws.amazon.com/blogs/machine-learning/apply-amazon-sagemaker-studio-lifecycle-configurations-using-aws-cdk/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build a read-through semantic cache with Amazon OpenSearch Serverless and Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_10/</link>
      <pubDate>Tue, 26 Nov 2024 17:05:50 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_10/</guid>
      <description>
        
          
            This post presents a strategy for optimizing LLM-based applications. Given the increasing need for efficient and cost-effective AI solutions, we present a serverless read-through caching blueprint that uses repeated data patterns. With this cache, developers can effectively save and access similar prompts, thereby enhancing their systems’ efficiency and response times.
Link to article: https://aws.amazon.com/blogs/machine-learning/build-a-read-through-semantic-cache-with-amazon-opensearch-serverless-and-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Rad AI reduces real-time inference latency by 50% using Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_11/</link>
      <pubDate>Tue, 26 Nov 2024 16:59:23 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_11/</guid>
      <description>
        
          
            This post is co-written with Ken Kao and Hasan Ali Demirci from Rad AI. Rad AI has reshaped radiology reporting, developing solutions that streamline the most tedious and repetitive tasks, and saving radiologists’ time. Since 2018, using state-of-the-art proprietary and open source large language models (LLMs), our flagship product—Rad AI Impressions— has significantly reduced the […]
Link to article: https://aws.amazon.com/blogs/machine-learning/rad-ai-reduces-real-time-inference-latency-by-50-using-amazon-sagemaker/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Read graphs, diagrams, tables, and scanned pages using multimodal prompts in Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_12/</link>
      <pubDate>Tue, 26 Nov 2024 16:48:42 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_12/</guid>
      <description>
        
          
            In this post, we demonstrate how to use models on Amazon Bedrock to retrieve information from images, tables, and scanned documents. We provide the following examples: 1/ performing object classification and object detection tasks, 2/ reading and querying graphs, and 3/ reading flowcharts and architecture diagrams (such as an AWS architecture diagram) and converting it to text.
Link to article: https://aws.amazon.com/blogs/machine-learning/read-graphs-diagrams-tables-and-scanned-pages-using-multimodal-prompts-in-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>How Crexi achieved ML models deployment on AWS at scale and boosted efficiency</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_14/</link>
      <pubDate>Tue, 26 Nov 2024 16:35:15 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_14/</guid>
      <description>
        
          
            Commercial Real Estate Exchange, Inc. (Crexi), is a digital marketplace and platform designed to streamline commercial real estate transactions. In this post, we will review how Crexi achieved its business needs and developed a versatile and powerful framework for AI/ML pipeline creation and deployment. This customizable and scalable solution allows its ML models to be efficiently deployed and managed to meet diverse project requirements.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-crexi-achieved-ml-models-deployment-on-aws-at-scale-and-boosted-efficiency/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Deploy Meta Llama 3.1 models cost-effectively in Amazon SageMaker JumpStart with AWS Inferentia and AWS Trainium</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_21/</link>
      <pubDate>Tue, 26 Nov 2024 00:37:42 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_21/</guid>
      <description>
        
          
            We’re excited to announce the availability of Meta Llama 3.1 8B and 70B inference support on AWS Trainium and AWS Inferentia instances in Amazon SageMaker JumpStart. Trainium and Inferentia, enabled by the AWS Neuron software development kit (SDK), offer high performance and lower the cost of deploying Meta Llama 3.1 by up to 50%. In this post, we demonstrate how to deploy Meta Llama 3.1 on Trainium and Inferentia instances in SageMaker JumpStart.
          
          
        
      </description>
    </item>
    
    <item>
      <title>AWS achieves ISO/IEC 42001:2023 Artificial Intelligence Management System accredited certification</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_22/</link>
      <pubDate>Tue, 26 Nov 2024 00:11:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_22/</guid>
      <description>
        
          
            Amazon Web Services (AWS) is excited to be the first major cloud service provider to announce ISO/IEC 42001 accredited certification for the following AI services: Amazon Bedrock, Amazon Q Business, Amazon Textract, and Amazon Transcribe. ISO/IEC 42001 is an international management system standard that outlines requirements and controls for organizations to promote the responsible development and use of AI systems.
Link to article: https://aws.amazon.com/blogs/machine-learning/aws-achieves-iso-iec-420012023-artificial-intelligence-management-system-accredited-certification/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>How 123RF saved over 90% of their translation costs by switching to Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_4/</link>
      <pubDate>Mon, 25 Nov 2024 16:01:36 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_4/</guid>
      <description>
        
          
            This post explores how 123RF used Amazon Bedrock, Anthropic’s Claude 3 Haiku, and a vector store to efficiently translate content metadata, significantly reduce costs, and improve their global content discovery capabilities.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-123rf-saved-over-90-of-their-translation-costs-by-switching-to-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Connect SharePoint Online to Amazon Q Business using OAuth 2.0 ROPC flow authentication</title>
      <link>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_5/</link>
      <pubDate>Mon, 25 Nov 2024 15:59:15 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_5/</guid>
      <description>
        
          
            In this post, we explore how to integrate Amazon Q Business with SharePoint Online using the OAuth 2.0 ROPC flow authentication method. We provide both manual and automated approaches using PowerShell scripts for configuring the required Azure AD settings. Additionally, we demonstrate how to enter those details along with your SharePoint authentication credentials into the Amazon Q console to finalize the secure connection.
Link to article: https://aws.amazon.com/blogs/machine-learning/connect-sharepoint-online-to-amazon-q-business-using-oauth-2-0-ropc-flow-authentication/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>John Snow Labs Medical LLMs are now available in Amazon SageMaker JumpStart</title>
      <link>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_6/</link>
      <pubDate>Mon, 25 Nov 2024 15:55:17 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_6/</guid>
      <description>
        
          
            Today, we are excited to announce that John Snow Labs’ Medical LLM – Small and Medical LLM – Medium large language models (LLMs) are now available on Amazon SageMaker Jumpstart. For medical doctors, this tool provides a rapid understanding of a patient’s medical journey, aiding in timely and informed decision-making from extensive documentation. This summarization capability not only boosts efficiency but also makes sure that no critical details are overlooked, thereby supporting optimal patient care and enhancing healthcare outcomes.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Accelerating Mixtral MoE fine-tuning on Amazon SageMaker with QLoRA</title>
      <link>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_0/</link>
      <pubDate>Fri, 22 Nov 2024 22:52:31 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_0/</guid>
      <description>
        
          
            In this post, we demonstrate how you can address the challenges of model customization being complex, time-consuming, and often expensive by using fully managed environment with Amazon SageMaker Training jobs to fine-tune the Mixtral 8x7B model using PyTorch Fully Sharded Data Parallel (FSDP) and Quantized Low Rank Adaptation (QLoRA).
Link to article: https://aws.amazon.com/blogs/machine-learning/accelerating-mixtral-moe-fine-tuning-on-amazon-sagemaker-with-qlora/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Amazon SageMaker Inference now supports G6e instances</title>
      <link>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_1/</link>
      <pubDate>Fri, 22 Nov 2024 19:56:49 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_1/</guid>
      <description>
        
          
            G6e instances on SageMaker unlock the ability to deploy a wide variety of open source models cost-effectively. With superior memory capacity, enhanced performance, and cost-effectiveness, these instances represent a compelling solution for organizations looking to deploy and scale their AI applications. The ability to handle larger models, support longer context lengths, and maintain high throughput makes G6e instances particularly valuable for modern AI applications.
Link to article: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-now-supports-g6e-instances/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Orchestrate generative AI workflows with Amazon Bedrock and AWS Step Functions</title>
      <link>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_2/</link>
      <pubDate>Fri, 22 Nov 2024 18:51:29 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_2/</guid>
      <description>
        
          
            This post discusses how to use AWS Step Functions to efficiently coordinate multi-step generative AI workflows, such as parallelizing API calls to Amazon Bedrock to quickly gather answers to lists of submitted questions. We also touch on the usage of Retrieval Augmented Generation (RAG) to optimize outputs and provide an extra layer of precision, as well as other possible integrations through Step Functions.
Link to article: https://aws.amazon.com/blogs/machine-learning/orchestrate-generative-ai-workflows-with-amazon-bedrock-and-aws-step-functions/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build generative AI applications on Amazon Bedrock with the AWS SDK for Python (Boto3)</title>
      <link>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_3/</link>
      <pubDate>Fri, 22 Nov 2024 18:43:36 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_3/</guid>
      <description>
        
          
            In this post, we demonstrate how to use Amazon Bedrock with the AWS SDK for Python (Boto3) to programmatically incorporate FMs. We explore invoking a specific FM and processing the generated text, showcasing the potential for developers to use these models in their applications for a variety of use cases
Link to article: https://aws.amazon.com/blogs/machine-learning/build-generative-ai-applications-on-amazon-bedrock-with-the-aws-sdk-for-python-boto3/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Improve factual consistency with LLM Debates</title>
      <link>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_4/</link>
      <pubDate>Fri, 22 Nov 2024 18:40:55 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_4/</guid>
      <description>
        
          
            In this post, we demonstrate the potential of large language model (LLM) debates using a supervised dataset with ground truth. In this post, we navigate the LLM debating technique with persuasive LLMs having two expert debater LLMs (Anthropic Claude 3 Sonnet and Mixtral 8X7B) and one judge LLM (Mistral 7B v2 to measure, compare, and contrast its performance against other techniques like self-consistency (with naive and expert judges) and LLM consultancy.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Governing the ML lifecycle at scale, Part 3: Setting up data governance at scale</title>
      <link>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_5/</link>
      <pubDate>Fri, 22 Nov 2024 18:28:16 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_5/</guid>
      <description>
        
          
            This post dives deep into how to set up data governance at scale using Amazon DataZone for the data mesh. The data mesh is a modern approach to data management that decentralizes data ownership and treats data as a product. It enables different business units within an organization to create, share, and govern their own data assets, promoting self-service analytics and reducing the time required to convert data experiments into production-ready applications.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Amazon Bedrock Flows is now generally available with enhanced safety and traceability</title>
      <link>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_7/</link>
      <pubDate>Fri, 22 Nov 2024 18:09:56 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_7/</guid>
      <description>
        
          
            Today, we are excited to announce the general availability of Amazon Bedrock Flows (previously known as Prompt Flows). With Bedrock Flows, you can quickly build and execute complex generative AI workflows without writing code. Bedrock Flows makes it easier for developers and businesses to harness the power of generative AI, enabling you to create more sophisticated and efficient AI-driven solutions for your customers.
Link to article: https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-flows-is-now-generally-available-with-enhanced-safety-and-traceability/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Implement secure API access to your Amazon Q Business applications with IAM federation user access management</title>
      <link>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_8/</link>
      <pubDate>Fri, 22 Nov 2024 18:02:28 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_11_2024/22_11_2024_8/</guid>
      <description>
        
          
            Amazon Q Business provides a rich set of APIs to perform administrative tasks and to build an AI assistant with customized user experience for your enterprise. In this post, we show how to use Amazon Q Business APIs when using AWS Identity and Access Management (IAM) federation for user access management.
Link to article: https://aws.amazon.com/blogs/machine-learning/implement-secure-api-access-to-your-amazon-q-business-applications-with-iam-federation-user-access-management/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Enhance speech synthesis and video generation models with RLHF using audio and video segmentation in Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/21_11_2024/21_11_2024_4/</link>
      <pubDate>Thu, 21 Nov 2024 17:27:01 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_11_2024/21_11_2024_4/</guid>
      <description>
        
          
            In this post, we show you how to implement an audio and video segmentation solution using SageMaker Ground Truth. We guide you through deploying the necessary infrastructure using AWS CloudFormation, creating an internal labeling workforce, and setting up your first labeling job. By the end of this post, you will have a fully functional audio/video segmentation workflow that you can adapt for various use cases, from training speech synthesis models to improving video generation capabilities.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Using responsible AI principles with Amazon Bedrock Batch Inference</title>
      <link>https://www.dotnetramblings.com/post/21_11_2024/21_11_2024_5/</link>
      <pubDate>Thu, 21 Nov 2024 17:23:53 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_11_2024/21_11_2024_5/</guid>
      <description>
        
          
            In this post, we explore a practical, cost-effective approach for incorporating responsible AI guardrails into Amazon Bedrock Batch Inference workflows. Although we use a call center’s transcript summarization as our primary example, the methods we discuss are broadly applicable to a variety of batch inference use cases where ethical considerations and data protection are a top priority.
Link to article: https://aws.amazon.com/blogs/machine-learning/using-responsible-ai-principles-with-amazon-bedrock-batch-inference/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Revolutionizing knowledge management: VW’s AI prototype journey with AWS</title>
      <link>https://www.dotnetramblings.com/post/21_11_2024/21_11_2024_6/</link>
      <pubDate>Thu, 21 Nov 2024 17:19:20 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_11_2024/21_11_2024_6/</guid>
      <description>
        
          
            we’re excited to share the journey of the VW—an innovator in the automotive industry and Europe’s largest car maker—to enhance knowledge management by using generative AI, Amazon Bedrock, and Amazon Kendra to devise a solution based on Retrieval Augmented Generation (RAG) that makes internal information more easily accessible by its users. This solution efficiently handles documents that include both text and images, significantly enhancing VW&#39;s knowledge management capabilities within their production domain.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Fine-tune large language models with Amazon SageMaker Autopilot</title>
      <link>https://www.dotnetramblings.com/post/21_11_2024/21_11_2024_7/</link>
      <pubDate>Thu, 21 Nov 2024 17:01:19 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_11_2024/21_11_2024_7/</guid>
      <description>
        
          
            Fine-tuning foundation models (FMs) is a process that involves exposing a pre-trained FM to task-specific data and fine-tuning its parameters. It can then develop a deeper understanding and produce more accurate and relevant outputs for that particular domain. In this post, we show how to use an Amazon SageMaker Autopilot training job with the AutoMLV2 […]
Link to article: https://aws.amazon.com/blogs/machine-learning/fine-tune-large-language-models-with-amazon-sagemaker-autopilot/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Unify structured data in Amazon Aurora and unstructured data in Amazon S3 for insights using Amazon Q</title>
      <link>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_3/</link>
      <pubDate>Wed, 20 Nov 2024 16:15:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_3/</guid>
      <description>
        
          
            In today’s data-intensive business landscape, organizations face the challenge of extracting valuable insights from diverse data sources scattered across their infrastructure. Whether it’s structured data in databases or unstructured content in document repositories, enterprises often struggle to efficiently query and use this wealth of information. In this post, we explore how you can use Amazon […]
Link to article: https://aws.amazon.com/blogs/machine-learning/unify-structured-data-in-amazon-aurora-and-unstructured-data-in-amazon-s3-for-insights-using-amazon-q/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Automate Q&amp;A email responses with Amazon Bedrock Knowledge Bases</title>
      <link>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_4/</link>
      <pubDate>Wed, 20 Nov 2024 16:11:36 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_4/</guid>
      <description>
        
          
            In this post, we illustrate automating the responses to email inquiries by using Amazon Bedrock Knowledge Bases and Amazon Simple Email Service (Amazon SES), both fully managed services. By linking user queries to relevant company domain information, Amazon Bedrock Knowledge Bases offers personalized responses.
Link to article: https://aws.amazon.com/blogs/machine-learning/automate-qa-email-responses-with-amazon-bedrock-knowledge-bases/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Streamline RAG applications with intelligent metadata filtering using Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_5/</link>
      <pubDate>Wed, 20 Nov 2024 16:08:18 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_5/</guid>
      <description>
        
          
            In this post, we explore an innovative approach that uses LLMs on Amazon Bedrock to intelligently extract metadata filters from natural language queries. By combining the capabilities of LLM function calling and Pydantic data models, you can dynamically extract metadata from user queries. This approach can also enhance the quality of retrieved information and responses generated by the RAG applications.
Link to article: https://aws.amazon.com/blogs/machine-learning/streamline-rag-applications-with-intelligent-metadata-filtering-using-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Embedding secure generative AI in mission-critical public safety applications</title>
      <link>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_7/</link>
      <pubDate>Wed, 20 Nov 2024 15:59:41 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_7/</guid>
      <description>
        
          
            This post shows how Mark43 uses Amazon Q Business to create a secure, generative AI-powered assistant that drives operational efficiency and improves community service. We explain how they embedded Amazon Q Business web experience in their web application with low code, so they could focus on creating a rich AI experience for their customers.
Link to article: https://aws.amazon.com/blogs/machine-learning/embedding-secure-generative-ai-in-mission-critical-public-safety-applications/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>How FP8 boosts LLM training by 18% on Amazon SageMaker P5 instances</title>
      <link>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_8/</link>
      <pubDate>Wed, 20 Nov 2024 15:54:16 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/20_11_2024/20_11_2024_8/</guid>
      <description>
        
          
            LLM training has seen remarkable advances in recent years, with organizations pushing the boundaries of what’s possible in terms of model size, performance, and efficiency. In this post, we explore how FP8 optimization can significantly speed up large model training on Amazon SageMaker P5 instances.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-fp8-boosts-llm-training-by-18-on-amazon-sagemaker-p5-instances/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
