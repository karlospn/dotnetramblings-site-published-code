<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Wed, 04 Dec 2024 19:28:56 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Amazon Bedrock Marketplace now includes NVIDIA models: Introducing NVIDIA Nemotron-4 NIM microservices</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_2/</link>
      <pubDate>Wed, 04 Dec 2024 19:28:56 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_2/</guid>
      <description>
        
          
            At AWS re:Invent 2024, we are excited to introduce Amazon Bedrock Marketplace. This a revolutionary new capability within Amazon Bedrock that serves as a centralized hub for discovering, testing, and implementing foundation models (FMs). In this post, we discuss the advantages and capabilities of Amazon Bedrock Marketplace and Nemotron models, and how to get started.
Link to article: https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-marketplace-now-includes-nvidia-models-introducing-nvidia-nemotron-4-nim-microservices/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Real value, real time: Production AI with Amazon SageMaker and Tecton</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_4/</link>
      <pubDate>Wed, 04 Dec 2024 19:01:36 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_4/</guid>
      <description>
        
          
            In this post, we discuss how Amazon SageMaker and Tecton work together to simplify the development and deployment of production-ready AI applications, particularly for real-time use cases like fraud detection. The integration enables faster time to value by abstracting away complex engineering tasks, allowing teams to focus on building features and use cases while providing a streamlined framework for both offline training and online serving of ML models.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Use Amazon Bedrock tooling with Amazon SageMaker JumpStart models</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_5/</link>
      <pubDate>Wed, 04 Dec 2024 18:42:03 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_5/</guid>
      <description>
        
          
            In this post, we explore how to deploy AI models from SageMaker JumpStart and use them with Amazon Bedrock&#39;s powerful features. Users can combine SageMaker JumpStart&#39;s model hosting with Bedrock&#39;s security and monitoring tools. We demonstrate this using the Gemma 2 9B Instruct model as an example, showing how to deploy it and use Bedrock&#39;s advanced capabilities.
Link to article: https://aws.amazon.com/blogs/machine-learning/use-amazon-bedrock-tooling-with-amazon-sagemaker-jumpstart-models/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>A guide to Amazon Bedrock Model Distillation (preview)</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_6/</link>
      <pubDate>Wed, 04 Dec 2024 18:39:31 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_6/</guid>
      <description>
        
          
            This post introduces the workflow of Amazon Bedrock Model Distillation. We first introduce the general concept of model distillation in Amazon Bedrock, and then focus on the important steps in model distillation, including setting up permissions, selecting the models, providing input dataset, commencing the model distillation jobs, and conducting evaluation and deployment of the student models after model distillation.
Link to article: https://aws.amazon.com/blogs/machine-learning/a-guide-to-amazon-bedrock-model-distillation-preview/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build generative AI applications quickly with Amazon Bedrock IDE in Amazon SageMaker Unified Studio</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_7/</link>
      <pubDate>Wed, 04 Dec 2024 18:36:49 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_7/</guid>
      <description>
        
          
            In this post, we&#39;ll show how anyone in your company can use Amazon Bedrock IDE to quickly create a generative AI chat agent application that analyzes sales performance data. Through simple conversations, business teams can use the chat agent to extract valuable insights from both structured and unstructured data sources without writing code or managing complex data pipelines.
Link to article: https://aws.amazon.com/blogs/machine-learning/build-generative-ai-applications-quickly-with-amazon-bedrock-ide-in-amazon-sagemaker-unified-studio/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Scale ML workflows with Amazon SageMaker Studio and Amazon SageMaker HyperPod</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_8/</link>
      <pubDate>Wed, 04 Dec 2024 18:31:15 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_8/</guid>
      <description>
        
          
            The integration of Amazon SageMaker Studio and Amazon SageMaker HyperPod offers a streamlined solution that provides data scientists and ML engineers with a comprehensive environment that supports the entire ML lifecycle, from development to deployment at scale. In this post, we walk you through the process of scaling your ML workloads using SageMaker Studio and SageMaker HyperPod.
Link to article: https://aws.amazon.com/blogs/machine-learning/scale-ml-workflows-with-amazon-sagemaker-studio-and-amazon-sagemaker-hyperpod/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Introducing Amazon Kendra GenAI Index – Enhanced semantic search and retrieval capabilities</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_10/</link>
      <pubDate>Wed, 04 Dec 2024 17:28:17 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_10/</guid>
      <description>
        
          
            Amazon has introduced the Amazon Kendra GenAI Index, a new offering designed to enhance semantic search and retrieval capabilities for enterprise AI applications. This index is optimized for Retrieval Augmented Generation (RAG) and intelligent search, allowing businesses to build more effective digital assistants and search experiences.
Link to article: https://aws.amazon.com/blogs/machine-learning/introducing-amazon-kendra-genai-index-enhanced-semantic-search-and-retrieval-capabilities/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Building Generative AI and ML solutions faster with AI apps from AWS partners using Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_11/</link>
      <pubDate>Wed, 04 Dec 2024 17:03:49 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_11/</guid>
      <description>
        
          
            Today, we’re excited to announce that AI apps from AWS Partners are now available in SageMaker. You can now find, deploy, and use these AI apps privately and securely, all without leaving SageMaker AI, so you can develop performant AI models faster.
Link to article: https://aws.amazon.com/blogs/machine-learning/building-generative-ai-and-ml-solutions-faster-with-ai-apps-from-aws-partners-using-amazon-sagemaker/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Query structured data from Amazon Q Business using Amazon QuickSight integration</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_2/</link>
      <pubDate>Tue, 03 Dec 2024 19:36:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_2/</guid>
      <description>
        
          
            In this post, we show how Amazon Q Business integrates with QuickSight to enable users to query both structured and unstructured data in a unified way. The integration allows users to connect to over 20 structured data sources like Amazon Redshift and PostgreSQL, while getting real-time answers with visualizations. Amazon Q Business combines information from structured sources through QuickSight with unstructured content to provide comprehensive answers to user queries.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Elevate customer experience by using the Amazon Q Business custom plugin for New Relic AI</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_3/</link>
      <pubDate>Tue, 03 Dec 2024 18:57:22 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_3/</guid>
      <description>
        
          
            The New Relic AI custom plugin for Amazon Q Business creates a unified solution that combines New Relic AI’s observability insights and recommendations and Amazon Q Business’s Retrieval Augmented Generation (RAG) capabilities, in and a natural language interface for east of use. This post explores the use case, how this custom plugin works, how it can be enabled, and how it can help elevate customers’ digital experiences.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Amazon SageMaker launches the updated inference optimization toolkit for generative AI</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_4/</link>
      <pubDate>Tue, 03 Dec 2024 18:55:05 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_4/</guid>
      <description>
        
          
            Today, Amazon SageMaker is excited to announce updates to the inference optimization toolkit, providing new functionality and enhancements to help you optimize generative AI models even faster.In this post, we discuss these new features of the toolkit in more detail.
Link to article: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-launches-the-updated-inference-optimization-toolkit-for-generative-ai/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Syngenta develops a generative AI assistant to support sales representatives using Amazon Bedrock Agents</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_5/</link>
      <pubDate>Tue, 03 Dec 2024 18:52:47 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_5/</guid>
      <description>
        
          
            In this post, we explore how Syngenta collaborated with AWS to develop Cropwise AI, a generative AI assistant powered by Amazon Bedrock Agents that helps sales representatives make better seed product recommendations to farmers across North America. The solution transforms the seed selection process by simplifying complex data into natural conversations, providing quick access to detailed seed product information, and enabling personalized recommendations at scale through a mobile app interface.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Speed up your AI inference workloads with new NVIDIA-powered capabilities in Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_14/</link>
      <pubDate>Tue, 03 Dec 2024 00:51:21 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_14/</guid>
      <description>
        
          
            At re:Invent 2024, we are excited to announce new capabilities to speed up your AI inference workloads with NVIDIA accelerated computing and software offerings on Amazon SageMaker. In this post, we will explore how you can use these new capabilities to enhance your AI inference on Amazon SageMaker. We&#39;ll walk through the process of deploying NVIDIA NIM microservices from AWS Marketplace for SageMaker Inference. We&#39;ll then dive into NVIDIA’s model offerings on SageMaker JumpStart, showcasing how to access and deploy the Nemotron-4 model directly in the JumpStart interface.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Unlock cost savings with the new scale down to zero feature in SageMaker Inference</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_15/</link>
      <pubDate>Tue, 03 Dec 2024 00:51:14 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_15/</guid>
      <description>
        
          
            Today at AWS re:Invent 2024, we are excited to announce a new feature for Amazon SageMaker inference endpoints: the ability to scale SageMaker inference endpoints to zero instances. This long-awaited capability is a game changer for our customers using the power of AI and machine learning (ML) inference in the cloud.
Link to article: https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Supercharge your auto scaling for generative AI inference – Introducing Container Caching in SageMaker Inference</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_16/</link>
      <pubDate>Tue, 03 Dec 2024 00:51:06 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_16/</guid>
      <description>
        
          
            Today at AWS re:Invent 2024, we are excited to announce the new Container Caching capability in Amazon SageMaker, which significantly reduces the time required to scale generative AI models for inference. This innovation allows you to scale your models faster, observing up to 56% reduction in latency when scaling a new model copy and up to 30% when adding a model copy on a new instance. In this post, we explore the new Container Caching feature for SageMaker inference, addressing the challenges of deploying and scaling large language models (LLMs).
          
          
        
      </description>
    </item>
    
    <item>
      <title>Introducing Fast Model Loader in SageMaker Inference: Accelerate autoscaling for your Large Language Models (LLMs) – part 1</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_17/</link>
      <pubDate>Tue, 03 Dec 2024 00:50:57 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_17/</guid>
      <description>
        
          
            Today at AWS re:Invent 2024, we are excited to announce a new capability in Amazon SageMaker Inference that significantly reduces the time required to deploy and scale LLMs for inference using LMI: Fast Model Loader. In this post, we delve into the technical details of Fast Model Loader, explore its integration with existing SageMaker workflows, discuss how you can get started with this powerful new feature, and share customer success stories.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Introducing Fast Model Loader in SageMaker Inference: Accelerate autoscaling for your Large Language Models (LLMs) – Part 2</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_18/</link>
      <pubDate>Tue, 03 Dec 2024 00:50:48 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_18/</guid>
      <description>
        
          
            In this post, we provide a detailed, hands-on guide to implementing Fast Model Loader in your LLM deployments. We explore two approaches: using the SageMaker Python SDK for programmatic implementation, and using the Amazon SageMaker Studio UI for a more visual, interactive experience. Whether you’re a developer who prefers working with code or someone who favors a graphical interface, you’ll learn how to take advantage of this powerful feature to accelerate your LLM deployments.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Fast and accurate zero-shot forecasting with Chronos-Bolt and AutoGluon</title>
      <link>https://www.dotnetramblings.com/post/02_12_2024/02_12_2024_1/</link>
      <pubDate>Mon, 02 Dec 2024 18:20:13 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/02_12_2024/02_12_2024_1/</guid>
      <description>
        
          
            Chronos models are available for Amazon SageMaker customers through AutoGluon-TimeSeries and Amazon SageMaker JumpStart. In this post, we introduce Chronos-Bolt, our latest FM for forecasting that has been integrated into AutoGluon-TimeSeries.
Link to article: https://aws.amazon.com/blogs/machine-learning/fast-and-accurate-zero-shot-forecasting-with-chronos-bolt-and-autogluon/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>How Amazon Finance Automation built a generative AI Q&amp;A chat assistant using Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/02_12_2024/02_12_2024_2/</link>
      <pubDate>Mon, 02 Dec 2024 18:11:18 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/02_12_2024/02_12_2024_2/</guid>
      <description>
        
          
            Amazon Finance Automation developed a large language model (LLM)-based question-answer chat assistant on Amazon Bedrock. This solution empowers analysts to rapidly retrieve answers to customer queries, generating prompt responses within the same communication thread. As a result, it drastically reduces the time required to address customer queries. In this post, we share how Amazon Finance Automation built this generative AI Q&amp;amp;A chat assistant using Amazon Bedrock.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-amazon-finance-automation-built-a-generative-ai-qa-chat-assistant-using-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Cohere Rerank 3.5 is now available in Amazon Bedrock through Rerank API</title>
      <link>https://www.dotnetramblings.com/post/01_12_2024/01_12_2024_0/</link>
      <pubDate>Sun, 01 Dec 2024 22:29:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/01_12_2024/01_12_2024_0/</guid>
      <description>
        
          
            We are excited to announce the availability of Cohere’s advanced reranking model Rerank 3.5 through our new Rerank API in Amazon Bedrock. This powerful reranking model enables AWS customers to significantly improve their search relevance and content ranking capabilities. In this post, we discuss the need for Reranking, the capabilities of Cohere’s Rerank 3.5, and how to get started using it on Amazon Bedrock.
Link to article: https://aws.amazon.com/blogs/machine-learning/cohere-rerank-3-5-is-now-available-in-amazon-bedrock-through-rerank-api/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>AWS DeepRacer: How to master physical racing?</title>
      <link>https://www.dotnetramblings.com/post/01_12_2024/01_12_2024_1/</link>
      <pubDate>Sun, 01 Dec 2024 22:20:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/01_12_2024/01_12_2024_1/</guid>
      <description>
        
          
            In this blog post, I will look at what makes physical AWS DeepRacer racing—a real car on a real track—different to racing in the virtual world—a model in a simulated 3D environment. I will cover the basics, the differences in virtual compared to physical, and what steps I have taken to get a deeper understanding of the challenge.
Link to article: https://aws.amazon.com/blogs/machine-learning/aws-deepracer-how-to-master-physical-racing/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Easily deploy and manage hundreds of LoRA adapters with SageMaker efficient multi-adapter inference</title>
      <link>https://www.dotnetramblings.com/post/29_11_2024/29_11_2024_0/</link>
      <pubDate>Fri, 29 Nov 2024 20:44:39 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_11_2024/29_11_2024_0/</guid>
      <description>
        
          
            The new efficient multi-adapter inference feature of Amazon SageMaker unlocks exciting possibilities for customers using fine-tuned models. This capability integrates with SageMaker inference components to allow you to deploy and manage hundreds of fine-tuned Low-Rank Adaptation (LoRA) adapters through SageMaker APIs. In this post, we show how to use the new efficient multi-adapter inference feature in SageMaker.
Link to article: https://aws.amazon.com/blogs/machine-learning/easily-deploy-and-manage-hundreds-of-lora-adapters-with-sagemaker-efficient-multi-adapter-inference/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Improve the performance of your Generative AI applications with Prompt Optimization on Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/29_11_2024/29_11_2024_1/</link>
      <pubDate>Fri, 29 Nov 2024 19:49:30 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_11_2024/29_11_2024_1/</guid>
      <description>
        
          
            Today, we are excited to announce the availability of Prompt Optimization on Amazon Bedrock. With this capability, you can now optimize your prompts for several use cases with a single API call or a click of a button on the Amazon Bedrock console. In this post, we discuss how you can get started with this new feature using an example use case in addition to discussing some performance benchmarks.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
