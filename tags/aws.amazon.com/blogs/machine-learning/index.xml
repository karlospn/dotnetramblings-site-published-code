<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Thu, 23 May 2024 19:08:31 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Accelerate Mixtral 8x7B pre-training with expert parallelism on Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/23_05_2024/23_05_2024_1/</link>
      <pubDate>Thu, 23 May 2024 19:08:31 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/23_05_2024/23_05_2024_1/</guid>
      <description>
        
          
            Mixture of Experts (MoE) architectures for large language models (LLMs) have recently gained popularity due to their ability to increase model capacity and computational efficiency compared to fully dense models. By utilizing sparse expert subnetworks that process different subsets of tokens, MoE models can effectively increase the number of parameters while requiring less computation per […]
Link to article: https://aws.amazon.com/blogs/machine-learning/accelerate-mixtral-8x7b-pre-training-with-expert-parallelism-on-amazon-sagemaker/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Generating fashion product descriptions by fine-tuning a vision-language model with SageMaker and Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/22_05_2024/22_05_2024_2/</link>
      <pubDate>Wed, 22 May 2024 19:33:45 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_05_2024/22_05_2024_2/</guid>
      <description>
        
          
            This post shows you how to predict domain-specific product attributes from product images by fine-tuning a VLM on a fashion dataset using Amazon SageMaker, and then using Amazon Bedrock to generate product descriptions using the predicted attributes as input. So you can follow along, we’re sharing the code in a GitHub repository.
Link to article: https://aws.amazon.com/blogs/machine-learning/generating-fashion-product-descriptions-by-fine-tuning-a-vision-language-model-with-sagemaker-and-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Create a multimodal assistant with advanced RAG and Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/21_05_2024/21_05_2024_4/</link>
      <pubDate>Tue, 21 May 2024 16:28:09 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_05_2024/21_05_2024_4/</guid>
      <description>
        
          
            In this post, we present a new approach named multimodal RAG (mmRAG) to tackle those existing limitations in greater detail. The solution intends to address these limitations for practical generative artificial intelligence (AI) assistant use cases. Additionally, we examine potential solutions to enhance the capabilities of large language models (LLMs) and visual language models (VLMs) with advanced LangChain capabilities, enabling them to generate more comprehensive, coherent, and accurate outputs while effectively handling multimodal data
          
          
        
      </description>
    </item>
    
    <item>
      <title>How 20 Minutes empowers journalists and boosts audience engagement with generative AI on Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/21_05_2024/21_05_2024_5/</link>
      <pubDate>Tue, 21 May 2024 16:16:28 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_05_2024/21_05_2024_5/</guid>
      <description>
        
          
            This post is co-written with Aurélien Capdecomme and Bertrand d’Aure from 20 Minutes. With 19 million monthly readers, 20 Minutes is a major player in the French media landscape. The media organization delivers useful, relevant, and accessible information to an audience that consists primarily of young and active urban readers. Every month, nearly 8.3 million 25–49-year-olds choose […]
Link to article: https://aws.amazon.com/blogs/machine-learning/how-20-minutes-empowers-journalists-and-boosts-audience-engagement-with-generative-ai-on-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Efficient and cost-effective multi-tenant LoRA serving with Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/21_05_2024/21_05_2024_13/</link>
      <pubDate>Tue, 21 May 2024 15:33:54 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_05_2024/21_05_2024_13/</guid>
      <description>
        
          
            In this post, we explore a solution that addresses these challenges head-on using LoRA serving with Amazon SageMaker. By using the new performance optimizations of LoRA techniques in SageMaker large model inference (LMI) containers along with inference components, we demonstrate how organizations can efficiently manage and serve their growing portfolio of fine-tuned models, while optimizing costs and providing seamless performance for their customers. The latest SageMaker LMI container offers unmerged-LoRA inference, sped up with our LMI-Dist inference engine and OpenAI style chat schema.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
