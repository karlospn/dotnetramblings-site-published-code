<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Thu, 26 Dec 2024 16:04:26 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Optimizing costs of generative AI applications on AWS</title>
      <link>https://www.dotnetramblings.com/post/26_12_2024/26_12_2024_0/</link>
      <pubDate>Thu, 26 Dec 2024 16:04:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_12_2024/26_12_2024_0/</guid>
      <description>
        
          
            Optimizing costs of generative AI applications on AWS is critical for realizing the full potential of this transformative technology. The post outlines key cost optimization pillars, including model selection and customization, token usage, inference pricing plans, and vector database considerations.
Link to article: https://aws.amazon.com/blogs/machine-learning/optimizing-costs-of-generative-ai-applications-on-aws/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>PEFT fine tuning of Llama 3 on SageMaker HyperPod with AWS Trainium</title>
      <link>https://www.dotnetramblings.com/post/24_12_2024/24_12_2024_1/</link>
      <pubDate>Tue, 24 Dec 2024 15:36:20 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_12_2024/24_12_2024_1/</guid>
      <description>
        
          
            In this blog post, we showcase how you can perform efficient supervised fine tuning for a Meta Llama 3 model using PEFT on AWS Trainium with SageMaker HyperPod. We use HuggingFace’s Optimum-Neuron software development kit (SDK) to apply LoRA to fine-tuning jobs, and use SageMaker HyperPod as the primary compute cluster to perform distributed training on Trainium. Using LoRA supervised fine-tuning for Meta Llama 3 models, you can further reduce your cost to fine tune models by up to 50% and reduce the training time by 70%.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Using transcription confidence scores to improve slot filling in Amazon Lex</title>
      <link>https://www.dotnetramblings.com/post/23_12_2024/23_12_2024_6/</link>
      <pubDate>Mon, 23 Dec 2024 14:06:37 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/23_12_2024/23_12_2024_6/</guid>
      <description>
        
          
            When building voice-enabled chatbots with Amazon Lex, one of the biggest challenges is accurately capturing user speech input for slot values. Transcription confidence scores can help ensure reliable slot filling. This blog post outlines strategies like progressive confirmation, adaptive re-prompting, and branching logic to create more robust slot filling experiences.
Link to article: https://aws.amazon.com/blogs/machine-learning/using-transcription-confidence-scores-to-improve-slot-filling-in-amazon-lex/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Improving Retrieval Augmented Generation accuracy with GraphRAG</title>
      <link>https://www.dotnetramblings.com/post/23_12_2024/23_12_2024_7/</link>
      <pubDate>Mon, 23 Dec 2024 13:56:47 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/23_12_2024/23_12_2024_7/</guid>
      <description>
        
          
            Lettria, an AWS Partner, demonstrated that integrating graph-based structures into RAG workflows improves answer precision by up to 35% compared to vector-only retrieval methods. In this post, we explore why GraphRAG is more comprehensive and explainable than vector RAG alone, and how you can use this approach using AWS services and Lettria.
Link to article: https://aws.amazon.com/blogs/machine-learning/improving-retrieval-augmented-generation-accuracy-with-graphrag/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Add a generative AI experience to your website or web application with Amazon Q embedded</title>
      <link>https://www.dotnetramblings.com/post/19_12_2024/19_12_2024_1/</link>
      <pubDate>Thu, 19 Dec 2024 15:27:45 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/19_12_2024/19_12_2024_1/</guid>
      <description>
        
          
            Amazon Q embedded is a feature that lets you embed a hosted Amazon Q Business assistant on your website or application to create more personalized experiences that boost end-users’ productivity. In this post, we demonstrate how to use the Amazon Q embedded feature to add an Amazon Q Business assistant to your website or web application using basic HTML or React.
Link to article: https://aws.amazon.com/blogs/machine-learning/add-a-generative-ai-experience-to-your-website-or-web-application-with-amazon-q-embedded/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>An introduction to preparing your own dataset for LLM training</title>
      <link>https://www.dotnetramblings.com/post/19_12_2024/19_12_2024_2/</link>
      <pubDate>Thu, 19 Dec 2024 15:23:46 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/19_12_2024/19_12_2024_2/</guid>
      <description>
        
          
            In this blog post, we provide an introduction to preparing your own dataset for LLM training. Whether your goal is to fine-tune a pre-trained model for a specific task or to continue pre-training for domain-specific applications, having a well-curated dataset is crucial for achieving optimal performance.
Link to article: https://aws.amazon.com/blogs/machine-learning/an-introduction-to-preparing-your-own-dataset-for-llm-training/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Design multi-agent orchestration with reasoning using Amazon Bedrock and open source frameworks</title>
      <link>https://www.dotnetramblings.com/post/19_12_2024/19_12_2024_3/</link>
      <pubDate>Thu, 19 Dec 2024 15:17:07 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/19_12_2024/19_12_2024_3/</guid>
      <description>
        
          
            This post provides step-by-step instructions for creating a collaborative multi-agent framework with reasoning capabilities to decouple business applications from FMs. It demonstrates how to combine Amazon Bedrock Agents with open source multi-agent frameworks, enabling collaborations and reasoning among agents to dynamically execute various tasks. The exercise will guide you through the process of building a reasoning orchestration system using Amazon Bedrock, Amazon Bedrock Knowledge Bases, Amazon Bedrock Agents, and FMs. We also explore the integration of Amazon Bedrock Agents with open source orchestration frameworks LangGraph and CrewAI for dispatching and reasoning.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
