<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Tue, 28 Jan 2025 17:42:39 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Develop a RAG-based application using Amazon Aurora with Amazon Kendra</title>
      <link>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_0/</link>
      <pubDate>Tue, 28 Jan 2025 17:42:39 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_0/</guid>
      <description>
        
          
            RAG retrieves data from a preexisting knowledge base (your data), combines it with the LLM’s knowledge, and generates responses with more human-like language. However, in order for generative AI to understand your data, some amount of data preparation is required, which involves a big learning curve. In this post, we walk you through how to convert your existing Aurora data into an index without needing data preparation for Amazon Kendra to perform data search and implement RAG that combines your data along with LLM knowledge to produce accurate responses.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Optimizing AI responsiveness: A practical guide to Amazon Bedrock latency-optimized inference</title>
      <link>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_1/</link>
      <pubDate>Tue, 28 Jan 2025 17:35:25 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_1/</guid>
      <description>
        
          
            In this post, we explore how Amazon Bedrock latency-optimized inference can help address the challenges of maintaining responsiveness in LLM applications. We&#39;ll dive deep into strategies for optimizing application performance and improving user experience. Whether you&#39;re building a new AI application or optimizing an existing one, you&#39;ll find practical guidance on both the technical aspects of latency optimization and real-world implementation approaches. We begin by explaining latency in LLM applications.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Track LLM model evaluation using Amazon SageMaker managed MLflow and FMEval</title>
      <link>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_2/</link>
      <pubDate>Tue, 28 Jan 2025 17:31:51 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_2/</guid>
      <description>
        
          
            In this post, we show how to use FMEval and Amazon SageMaker to programmatically evaluate LLMs. FMEval is an open source LLM evaluation library, designed to provide data scientists and machine learning (ML) engineers with a code-first experience to evaluate LLMs for various aspects, including accuracy, toxicity, fairness, robustness, and efficiency.
Link to article: https://aws.amazon.com/blogs/machine-learning/track-llm-model-evaluation-using-amazon-sagemaker-managed-mlflow-and-fmeval/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Create a SageMaker inference endpoint with custom model &amp; extended container</title>
      <link>https://www.dotnetramblings.com/post/27_01_2025/27_01_2025_1/</link>
      <pubDate>Mon, 27 Jan 2025 17:35:18 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_01_2025/27_01_2025_1/</guid>
      <description>
        
          
            This post walks you through the end-to-end process of deploying a single custom model on SageMaker using NASA’s Prithvi model. The Prithvi model is a first-of-its-kind temporal Vision transformer pre-trained by the IBM and NASA team on contiguous US Harmonised Landsat Sentinel 2 (HLS) data. It can be finetuned for image segmentation using the mmsegmentation library for use cases like burn scars detection, flood mapping, and multi-temporal crop classification.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Image and video prompt engineering for Amazon Nova Canvas and Amazon Nova Reel</title>
      <link>https://www.dotnetramblings.com/post/27_01_2025/27_01_2025_2/</link>
      <pubDate>Mon, 27 Jan 2025 17:14:59 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_01_2025/27_01_2025_2/</guid>
      <description>
        
          
            Amazon has introduced two new creative content generation models on Amazon Bedrock: Amazon Nova Canvas for image generation and Amazon Nova Reel for video creation. These models transform text and image inputs into custom visuals, opening up creative opportunities for both professional and personal projects. Nova Canvas, a state-of-the-art image generation model, creates professional-grade images […]
Link to article: https://aws.amazon.com/blogs/machine-learning/image-and-video-prompt-engineering-for-amazon-nova-canvas-and-amazon-nova-reel/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Security best practices to consider while fine-tuning models in Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/24_01_2025/24_01_2025_0/</link>
      <pubDate>Fri, 24 Jan 2025 17:02:53 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_01_2025/24_01_2025_0/</guid>
      <description>
        
          
            In this post, we implemented secure fine-tuning jobs in Amazon Bedrock, which is crucial for protecting sensitive data and maintaining the integrity of your AI models. By following the best practices outlined in this post, including proper IAM role configuration, encryption at rest and in transit, and network isolation, you can significantly enhance the security posture of your fine-tuning processes.
Link to article: https://aws.amazon.com/blogs/machine-learning/security-best-practices-to-consider-while-fine-tuning-models-in-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Secure a generative AI assistant with OWASP Top 10 mitigation</title>
      <link>https://www.dotnetramblings.com/post/24_01_2025/24_01_2025_1/</link>
      <pubDate>Fri, 24 Jan 2025 15:38:05 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_01_2025/24_01_2025_1/</guid>
      <description>
        
          
            In this post, we show you an example of a generative AI assistant application and demonstrate how to assess its security posture using the OWASP Top 10 for Large Language Model Applications, as well as how to apply mitigations for common threats.
Link to article: https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Streamline custom environment provisioning for Amazon SageMaker Studio: An automated CI/CD pipeline approach</title>
      <link>https://www.dotnetramblings.com/post/23_01_2025/23_01_2025_2/</link>
      <pubDate>Thu, 23 Jan 2025 18:15:10 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/23_01_2025/23_01_2025_2/</guid>
      <description>
        
          
            In this post, we show how to create an automated continuous integration and delivery (CI/CD) pipeline solution to build, scan, and deploy custom Docker images to SageMaker Studio domains. You can use this solution to promote consistency of the analytical environments for data science teams across your enterprise.
Link to article: https://aws.amazon.com/blogs/machine-learning/streamline-custom-environment-provisioning-for-amazon-sagemaker-studio-an-automated-ci-cd-pipeline-approach/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Enhance your customer’s omnichannel experience with Amazon Bedrock and Amazon Lex</title>
      <link>https://www.dotnetramblings.com/post/23_01_2025/23_01_2025_5/</link>
      <pubDate>Thu, 23 Jan 2025 16:50:06 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/23_01_2025/23_01_2025_5/</guid>
      <description>
        
          
            In this post, we show you how to set up Amazon Lex for an omnichannel chatbot experience and Amazon Bedrock to be your secondary validation layer. This allows your customers to potentially provide out-of-band responses both at the intent and slot collection levels without having to be re-prompted, allowing for a seamless customer experience.
Link to article: https://aws.amazon.com/blogs/machine-learning/enhance-your-customers-omnichannel-experience-with-amazon-bedrock-and-amazon-lex/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Introducing multi-turn conversation with an agent node for Amazon Bedrock Flows (preview)</title>
      <link>https://www.dotnetramblings.com/post/22_01_2025/22_01_2025_1/</link>
      <pubDate>Wed, 22 Jan 2025 21:16:22 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_01_2025/22_01_2025_1/</guid>
      <description>
        
          
            Today, we’re excited to announce multi-turn conversation with an agent node (preview), a powerful new capability in Flows. This new capability enhances the agent node functionality, enabling dynamic, back-and-forth conversations between users and flows, similar to a natural dialogue in a flow execution.
Link to article: https://aws.amazon.com/blogs/machine-learning/introducing-multi-turn-conversation-with-an-agent-node-for-amazon-bedrock-flows-preview/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Video security analysis for privileged access management using generative AI and Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/22_01_2025/22_01_2025_2/</link>
      <pubDate>Wed, 22 Jan 2025 19:37:24 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_01_2025/22_01_2025_2/</guid>
      <description>
        
          
            In this post, we show you an innovative solution to a challenge faced by security teams in highly regulated industries: the efficient security analysis of vast amounts of video recordings from Privileged Access Management (PAM) systems. We demonstrate how you can use Anthropic’s Claude 3 family of models and Amazon Bedrock to perform the complex task of analyzing video recordings of server console sessions and perform queries to highlight any potential security anomalies.
          
          
        
      </description>
    </item>
    
    <item>
      <title>How Cato Networks uses Amazon Bedrock to transform free text search into structured GraphQL queries</title>
      <link>https://www.dotnetramblings.com/post/22_01_2025/22_01_2025_4/</link>
      <pubDate>Wed, 22 Jan 2025 17:35:58 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/22_01_2025/22_01_2025_4/</guid>
      <description>
        
          
            Accurately converting free text inputs into structured data is crucial for applications that involve data management and user interaction. In this post, we introduce a real business use case from Cato Networks that significantly improved user experience. By using Amazon Bedrock, we gained access to state-of-the-art generative language models with built-in support for JSON schemas and structured data.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-cato-networks-uses-amazon-bedrock-to-transform-free-text-search-into-structured-graphql-queries/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Solve forecasting challenges for the retail and CPG industry using Amazon SageMaker Canvas</title>
      <link>https://www.dotnetramblings.com/post/21_01_2025/21_01_2025_0/</link>
      <pubDate>Tue, 21 Jan 2025 20:13:38 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_01_2025/21_01_2025_0/</guid>
      <description>
        
          
            In this post, we show you how Amazon Web Services (AWS) helps in solving forecasting challenges by customizing machine learning (ML) models for forecasting. We dive into Amazon SageMaker Canvas and explain how SageMaker Canvas can solve forecasting challenges for retail and consumer packaged goods (CPG) enterprises.
Link to article: https://aws.amazon.com/blogs/machine-learning/solve-forecasting-challenges-for-the-retail-and-cpg-industry-using-amazon-sagemaker-canvas/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Enabling generative AI self-service using Amazon Lex, Amazon Bedrock, and ServiceNow</title>
      <link>https://www.dotnetramblings.com/post/21_01_2025/21_01_2025_1/</link>
      <pubDate>Tue, 21 Jan 2025 20:10:55 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_01_2025/21_01_2025_1/</guid>
      <description>
        
          
            In this post, we show how you can integrate Amazon Lex with Amazon Bedrock Knowledge Bases and ServiceNow to provide 24/7 automated support and self-service options.
Link to article: https://aws.amazon.com/blogs/machine-learning/enabling-generative-ai-self-service-using-amazon-lex-amazon-bedrock-and-servicenow/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
