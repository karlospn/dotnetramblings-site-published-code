<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Thu, 26 Dec 2024 16:04:26 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Optimizing costs of generative AI applications on AWS</title>
      <link>https://www.dotnetramblings.com/post/26_12_2024/26_12_2024_0/</link>
      <pubDate>Thu, 26 Dec 2024 16:04:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_12_2024/26_12_2024_0/</guid>
      <description>
        
          
            Optimizing costs of generative AI applications on AWS is critical for realizing the full potential of this transformative technology. The post outlines key cost optimization pillars, including model selection and customization, token usage, inference pricing plans, and vector database considerations.
Link to article: https://aws.amazon.com/blogs/machine-learning/optimizing-costs-of-generative-ai-applications-on-aws/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>PEFT fine tuning of Llama 3 on SageMaker HyperPod with AWS Trainium</title>
      <link>https://www.dotnetramblings.com/post/24_12_2024/24_12_2024_1/</link>
      <pubDate>Tue, 24 Dec 2024 15:36:20 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_12_2024/24_12_2024_1/</guid>
      <description>
        
          
            In this blog post, we showcase how you can perform efficient supervised fine tuning for a Meta Llama 3 model using PEFT on AWS Trainium with SageMaker HyperPod. We use HuggingFaceâ€™s Optimum-Neuron software development kit (SDK) to apply LoRA to fine-tuning jobs, and use SageMaker HyperPod as the primary compute cluster to perform distributed training on Trainium. Using LoRA supervised fine-tuning for Meta Llama 3 models, you can further reduce your cost to fine tune models by up to 50% and reduce the training time by 70%.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
