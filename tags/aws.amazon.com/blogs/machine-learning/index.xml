<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Mon, 03 Feb 2025 17:04:42 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Accelerate video Q&amp;A workflows using Amazon Bedrock Knowledge Bases, Amazon Transcribe, and thoughtful UX design</title>
      <link>https://www.dotnetramblings.com/post/03_02_2025/03_02_2025_0/</link>
      <pubDate>Mon, 03 Feb 2025 17:04:42 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_02_2025/03_02_2025_0/</guid>
      <description>
        
          
            The solution presented in this post demonstrates a powerful pattern for accelerating video and audio review workflows while maintaining human oversight. By combining the power of AI models in Amazon Bedrock with human expertise, you can create tools that not only boost productivity but also maintain the critical element of human judgment in important decision-making processes.
Link to article: https://aws.amazon.com/blogs/machine-learning/accelerate-video-qa-workflows-using-amazon-bedrock-knowledge-bases-amazon-transcribe-and-thoughtful-ux-design/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Boost team innovation, productivity, and knowledge sharing with Amazon Q Apps</title>
      <link>https://www.dotnetramblings.com/post/03_02_2025/03_02_2025_1/</link>
      <pubDate>Mon, 03 Feb 2025 16:59:17 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_02_2025/03_02_2025_1/</guid>
      <description>
        
          
            In this post, we demonstrate how Amazon Q Apps can help maximize the value of existing knowledge resources and improve productivity among various teams, ranging from finance to DevOps to support engineers. We share specific examples of how the generative AI assistant can enable surface relevant information, distill complex topics, generate custom content, and execute workflows—all while maintaining robust security and data governance controls.
Link to article: https://aws.amazon.com/blogs/machine-learning/boost-team-innovation-productivity-and-knowledge-sharing-with-amazon-q-apps/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Harnessing Amazon Bedrock generative AI for resilient supply chain</title>
      <link>https://www.dotnetramblings.com/post/31_01_2025/31_01_2025_2/</link>
      <pubDate>Fri, 31 Jan 2025 19:59:48 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/31_01_2025/31_01_2025_2/</guid>
      <description>
        
          
            By leveraging the generative AI capabilities and tooling of Amazon Bedrock, you can create an intelligent nerve center that connects diverse data sources, converts data into actionable insights, and creates a comprehensive plan to mitigate supply chain risks. This post walks through how Amazon Bedrock Flows connects your business systems, monitors medical device shortages, and provides mitigation strategies based on knowledge from Amazon Bedrock Knowledge Bases or data stored in Amazon S3 directly.
          
          
        
      </description>
    </item>
    
    <item>
      <title>How Travelers Insurance classified emails with Amazon Bedrock and prompt engineering</title>
      <link>https://www.dotnetramblings.com/post/31_01_2025/31_01_2025_4/</link>
      <pubDate>Fri, 31 Jan 2025 17:18:15 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/31_01_2025/31_01_2025_4/</guid>
      <description>
        
          
            In this post, we discuss how FMs can reliably automate the classification of insurance service emails through prompt engineering. When formulating the problem as a classification task, an FM can perform well enough for production environments, while maintaining extensibility into other tasks and getting up and running quickly. All experiments were conducted using Anthropic’s Claude models on Amazon Bedrock.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-travelers-insurance-classified-emails-with-amazon-bedrock-and-prompt-engineering/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Accelerate digital pathology slide annotation workflows on AWS using H-optimus-0</title>
      <link>https://www.dotnetramblings.com/post/31_01_2025/31_01_2025_5/</link>
      <pubDate>Fri, 31 Jan 2025 17:10:08 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/31_01_2025/31_01_2025_5/</guid>
      <description>
        
          
            In this post, we demonstrate how to use H-optimus-0 for two common digital pathology tasks: patch-level analysis for detailed tissue examination, and slide-level analysis for broader diagnostic assessment. Through practical examples, we show you how to adapt this FM to these specific use cases while optimizing computational resources.
Link to article: https://aws.amazon.com/blogs/machine-learning/accelerate-digital-pathology-slide-annotation-workflows-on-aws-using-h-optimus-0/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>DeepSeek-R1 model now available in Amazon Bedrock Marketplace and Amazon SageMaker JumpStart</title>
      <link>https://www.dotnetramblings.com/post/31_01_2025/31_01_2025_10/</link>
      <pubDate>Fri, 31 Jan 2025 02:31:38 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/31_01_2025/31_01_2025_10/</guid>
      <description>
        
          
            DeepSeek-R1 is an advanced large language model that combines reinforcement learning, chain-of-thought reasoning, and a Mixture of Experts architecture to deliver efficient, interpretable responses while maintaining safety through Amazon Bedrock Guardrails integration.
Link to article: https://aws.amazon.com/blogs/machine-learning/deepseek-r1-model-now-available-in-amazon-bedrock-marketplace-and-amazon-sagemaker-jumpstart/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Streamline grant proposal reviews using Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/30_01_2025/30_01_2025_7/</link>
      <pubDate>Thu, 30 Jan 2025 17:55:52 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/30_01_2025/30_01_2025_7/</guid>
      <description>
        
          
            The AWS Social Responsibility &amp;amp; Impact (SRI) team recognized an opportunity to augment this function using generative AI. The team developed an innovative solution to streamline grant proposal review and evaluation by using the natural language processing (NLP) capabilities of Amazon Bedrock. In this post, we explore the technical implementation details and key learnings from the team’s Amazon Bedrock powered grant proposal review solution, providing a blueprint for organizations seeking to optimize their grants management processes.
          
          
        
      </description>
    </item>
    
    <item>
      <title>How Aetion is using generative AI and Amazon Bedrock to unlock hidden insights about patient populations</title>
      <link>https://www.dotnetramblings.com/post/30_01_2025/30_01_2025_8/</link>
      <pubDate>Thu, 30 Jan 2025 17:51:51 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/30_01_2025/30_01_2025_8/</guid>
      <description>
        
          
            In this post, we review how Aetion’s Smart Subgroups Interpreter enables users to interact with Smart Subgroups using natural language queries. Powered by Amazon Bedrock and Anthropic’s Claude 3 large language models (LLMs), the interpreter responds to user questions expressed in conversational language about patient subgroups and provides insights to generate further hypotheses and evidence.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-aetion-is-using-generative-ai-and-amazon-bedrock-to-unlock-hidden-insights-about-patient-populations/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Deploy DeepSeek-R1 distilled Llama models with Amazon Bedrock Custom Model Import</title>
      <link>https://www.dotnetramblings.com/post/30_01_2025/30_01_2025_15/</link>
      <pubDate>Thu, 30 Jan 2025 01:10:15 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/30_01_2025/30_01_2025_15/</guid>
      <description>
        
          
            In this post, we demonstrate how to deploy distilled versions of DeepSeek-R1 models using Amazon Bedrock Custom Model Import. We focus on importing the variants currently supported DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-Distill-Llama-70B, which offer an optimal balance between performance and resource efficiency.
Link to article: https://aws.amazon.com/blogs/machine-learning/deploy-deepseek-r1-distilled-llama-models-with-amazon-bedrock-custom-model-import/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Generative AI operating models in enterprise organizations with Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/29_01_2025/29_01_2025_0/</link>
      <pubDate>Wed, 29 Jan 2025 21:11:50 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_01_2025/29_01_2025_0/</guid>
      <description>
        
          
            As generative AI adoption grows, organizations should establish a generative AI operating model. An operating model defines the organizational design, core processes, technologies, roles and responsibilities, governance structures, and financial models that drive a business’s operations. In this post, we evaluate different generative AI operating model architectures that could be adopted.
Link to article: https://aws.amazon.com/blogs/machine-learning/generative-ai-operating-models-in-enterprise-organizations-with-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Develop a RAG-based application using Amazon Aurora with Amazon Kendra</title>
      <link>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_2/</link>
      <pubDate>Tue, 28 Jan 2025 17:42:39 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_2/</guid>
      <description>
        
          
            RAG retrieves data from a preexisting knowledge base (your data), combines it with the LLM’s knowledge, and generates responses with more human-like language. However, in order for generative AI to understand your data, some amount of data preparation is required, which involves a big learning curve. In this post, we walk you through how to convert your existing Aurora data into an index without needing data preparation for Amazon Kendra to perform data search and implement RAG that combines your data along with LLM knowledge to produce accurate responses.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Optimizing AI responsiveness: A practical guide to Amazon Bedrock latency-optimized inference</title>
      <link>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_3/</link>
      <pubDate>Tue, 28 Jan 2025 17:35:25 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_3/</guid>
      <description>
        
          
            In this post, we explore how Amazon Bedrock latency-optimized inference can help address the challenges of maintaining responsiveness in LLM applications. We&#39;ll dive deep into strategies for optimizing application performance and improving user experience. Whether you&#39;re building a new AI application or optimizing an existing one, you&#39;ll find practical guidance on both the technical aspects of latency optimization and real-world implementation approaches. We begin by explaining latency in LLM applications.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Track LLM model evaluation using Amazon SageMaker managed MLflow and FMEval</title>
      <link>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_4/</link>
      <pubDate>Tue, 28 Jan 2025 17:31:51 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/28_01_2025/28_01_2025_4/</guid>
      <description>
        
          
            In this post, we show how to use FMEval and Amazon SageMaker to programmatically evaluate LLMs. FMEval is an open source LLM evaluation library, designed to provide data scientists and machine learning (ML) engineers with a code-first experience to evaluate LLMs for various aspects, including accuracy, toxicity, fairness, robustness, and efficiency.
Link to article: https://aws.amazon.com/blogs/machine-learning/track-llm-model-evaluation-using-amazon-sagemaker-managed-mlflow-and-fmeval/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Create a SageMaker inference endpoint with custom model &amp; extended container</title>
      <link>https://www.dotnetramblings.com/post/27_01_2025/27_01_2025_1/</link>
      <pubDate>Mon, 27 Jan 2025 17:35:18 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_01_2025/27_01_2025_1/</guid>
      <description>
        
          
            This post walks you through the end-to-end process of deploying a single custom model on SageMaker using NASA’s Prithvi model. The Prithvi model is a first-of-its-kind temporal Vision transformer pre-trained by the IBM and NASA team on contiguous US Harmonised Landsat Sentinel 2 (HLS) data. It can be finetuned for image segmentation using the mmsegmentation library for use cases like burn scars detection, flood mapping, and multi-temporal crop classification.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Image and video prompt engineering for Amazon Nova Canvas and Amazon Nova Reel</title>
      <link>https://www.dotnetramblings.com/post/27_01_2025/27_01_2025_2/</link>
      <pubDate>Mon, 27 Jan 2025 17:14:59 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_01_2025/27_01_2025_2/</guid>
      <description>
        
          
            Amazon has introduced two new creative content generation models on Amazon Bedrock: Amazon Nova Canvas for image generation and Amazon Nova Reel for video creation. These models transform text and image inputs into custom visuals, opening up creative opportunities for both professional and personal projects. Nova Canvas, a state-of-the-art image generation model, creates professional-grade images […]
Link to article: https://aws.amazon.com/blogs/machine-learning/image-and-video-prompt-engineering-for-amazon-nova-canvas-and-amazon-nova-reel/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
