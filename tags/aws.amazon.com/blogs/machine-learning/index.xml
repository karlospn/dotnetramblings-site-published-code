<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Wed, 12 Feb 2025 17:44:10 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fine-tune LLMs with synthetic data for context-based Q&amp;A using Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/12_02_2025/12_02_2025_1/</link>
      <pubDate>Wed, 12 Feb 2025 17:44:10 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/12_02_2025/12_02_2025_1/</guid>
      <description>
        
          
            In this post, we explore how to use Amazon Bedrock to generate synthetic training data to fine-tune an LLM. Additionally, we provide concrete evaluation results that showcase the power of synthetic data in fine-tuning when data is scarce.
Link to article: https://aws.amazon.com/blogs/machine-learning/fine-tune-llms-with-synthetic-data-for-context-based-qa-using-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Achieve ~2x speed-up in LLM inference with Medusa-1 on Amazon SageMaker AI</title>
      <link>https://www.dotnetramblings.com/post/12_02_2025/12_02_2025_2/</link>
      <pubDate>Wed, 12 Feb 2025 17:41:33 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/12_02_2025/12_02_2025_2/</guid>
      <description>
        
          
            Researchers developed Medusa, a framework to speed up LLM inference by adding extra heads to predict multiple tokens simultaneously. This post demonstrates how to use Medusa-1, the first version of the framework, to speed up an LLM by fine-tuning it on Amazon SageMaker AI and confirms the speed up with deployment and a simple load test. Medusa-1 achieves an inference speedup of around two times without sacrificing model quality, with the exact improvement varying based on model size and data used.
          
          
        
      </description>
    </item>
    
    <item>
      <title>LLM-as-a-judge on Amazon Bedrock Model Evaluation</title>
      <link>https://www.dotnetramblings.com/post/12_02_2025/12_02_2025_3/</link>
      <pubDate>Wed, 12 Feb 2025 17:36:57 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/12_02_2025/12_02_2025_3/</guid>
      <description>
        
          
            This blog post explores LLM-as-a-judge on Amazon Bedrock Model Evaluation, providing comprehensive guidance on feature setup, evaluating job initiation through both the console and Python SDK and APIs, and demonstrating how this innovative evaluation feature can enhance generative AI applications across multiple metric categories including quality, user experience, instruction following, and safety.
Link to article: https://aws.amazon.com/blogs/machine-learning/llm-as-a-judge-on-amazon-bedrock-model-evaluation/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>From concept to reality: Navigating the Journey of RAG from proof of concept to production</title>
      <link>https://www.dotnetramblings.com/post/12_02_2025/12_02_2025_5/</link>
      <pubDate>Wed, 12 Feb 2025 17:27:52 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/12_02_2025/12_02_2025_5/</guid>
      <description>
        
          
            In this post, we explore the movement of RAG applications from their proof of concept or minimal viable product (MVP) phase to full-fledged production systems. When transitioning a RAG application from a proof of concept to a production-ready system, optimization becomes crucial to make sure the solution is reliable, cost-effective, and high-performing.
Link to article: https://aws.amazon.com/blogs/machine-learning/from-concept-to-reality-navigating-the-journey-of-rag-from-proof-of-concept-to-production/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Meta SAM 2.1 is now available in Amazon SageMaker JumpStart</title>
      <link>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_0/</link>
      <pubDate>Tue, 11 Feb 2025 23:09:11 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_0/</guid>
      <description>
        
          
            We are excited to announce that Meta’s Segment Anything Model (SAM) 2.1 vision segmentation model is publicly available through Amazon SageMaker JumpStart to deploy and run inference. Meta SAM 2.1 provides state-of-the-art video and image segmentation capabilities in a single model. In this post, we explored how SageMaker JumpStart empowers data scientists and ML engineers to discover, access, and deploy a wide range of pre-trained FMs for inference, including Meta’s most advanced and capable models to date.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Falcon 3 models now available in Amazon SageMaker JumpStart</title>
      <link>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_1/</link>
      <pubDate>Tue, 11 Feb 2025 22:16:27 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_1/</guid>
      <description>
        
          
            We are excited to announce that the Falcon 3 family of models from TII are available in Amazon SageMaker JumpStart. In this post, we explore how to deploy this model efficiently on Amazon SageMaker AI.
Link to article: https://aws.amazon.com/blogs/machine-learning/falcon-3-models-now-available-in-amazon-sagemaker-jumpstart/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Building a virtual meteorologist using Amazon Bedrock Agents</title>
      <link>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_3/</link>
      <pubDate>Tue, 11 Feb 2025 20:53:04 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_3/</guid>
      <description>
        
          
            In this post, we present a streamlined approach to deploying an AI-powered agent by combining Amazon Bedrock Agents and a foundation model (FM). We guide you through the process of configuring the agent and implementing the specific logic required for the virtual meteorologist to provide accurate weather-related responses.
Link to article: https://aws.amazon.com/blogs/machine-learning/building-a-virtual-meteorologist-using-amazon-bedrock-agents/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Amazon Q Business simplifies integration of enterprise knowledge bases at scale</title>
      <link>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_11/</link>
      <pubDate>Tue, 11 Feb 2025 17:11:32 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_11/</guid>
      <description>
        
          
            In this post, we demonstrate how to build a knowledge base solution by integrating enterprise data with Amazon Q Business using Amazon S3. This approach helps organizations improve operational efficiency, reduce response times, and gain valuable insights from their historical data. The solution uses AWS security best practices to promote data protection while enabling teams to create a comprehensive knowledge base from various data sources.
Link to article: https://aws.amazon.com/blogs/machine-learning/amazon-q-business-simplifies-integration-of-enterprise-knowledge-bases-at-scale/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Faster distributed graph neural network training with GraphStorm v0.4</title>
      <link>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_12/</link>
      <pubDate>Tue, 11 Feb 2025 17:03:16 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/11_02_2025/11_02_2025_12/</guid>
      <description>
        
          
            GraphStorm is a low-code enterprise graph machine learning (ML) framework that provides ML practitioners a simple way of building, training, and deploying graph ML solutions on industry-scale graph data. In this post, we demonstrate how GraphBolt enhances GraphStorm’s performance in distributed settings. We provide a hands-on example of using GraphStorm with GraphBolt on SageMaker for distributed training. Lastly, we share how to use Amazon SageMaker Pipelines with GraphStorm.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Transforming credit decisions using generative AI with Rich Data Co and AWS</title>
      <link>https://www.dotnetramblings.com/post/10_02_2025/10_02_2025_1/</link>
      <pubDate>Mon, 10 Feb 2025 20:05:34 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/10_02_2025/10_02_2025_1/</guid>
      <description>
        
          
            The mission of Rich Data Co (RDC) is to broaden access to sustainable credit globally. Its software-as-a-service (SaaS) solution empowers leading banks and lenders with deep customer insights and AI-driven decision-making capabilities. In this post, we discuss how RDC uses generative AI on Amazon Bedrock to build these assistants and accelerate its overall mission of democratizing access to sustainable credit.
Link to article: https://aws.amazon.com/blogs/machine-learning/transforming-credit-decisions-using-generative-ai-with-rich-data-co-and-aws/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build agentic AI solutions with DeepSeek-R1, CrewAI, and Amazon SageMaker AI</title>
      <link>https://www.dotnetramblings.com/post/10_02_2025/10_02_2025_2/</link>
      <pubDate>Mon, 10 Feb 2025 19:33:10 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/10_02_2025/10_02_2025_2/</guid>
      <description>
        
          
            In this post, we demonstrate how you can deploy an LLM such as DeepSeek-R1—or another FM of your choice—from popular model hubs like SageMaker JumpStart or Hugging Face Hub to SageMaker AI for real-time inference. We explore inference frameworks like Hugging Face TGI which helps streamline deployment while integrating built-in performance optimizations to minimize latency and maximize throughput. Additionally, we showcase how the SageMaker developer-friendly Python SDK simplifies endpoint orchestration, allowing seamless experimentation and scaling of LLM-powered applications.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Automate bulk image editing with Crop.photo and Amazon Rekognition</title>
      <link>https://www.dotnetramblings.com/post/10_02_2025/10_02_2025_4/</link>
      <pubDate>Mon, 10 Feb 2025 18:50:29 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/10_02_2025/10_02_2025_4/</guid>
      <description>
        
          
            In this post, we explore how Crop.photo uses Amazon Rekognition to provide sophisticated image analysis, enabling automated and precise editing of large volumes of images. This integration streamlines the image editing process for clients, providing speed and accuracy, which is crucial in the fast-paced environments of ecommerce and sports.
Link to article: https://aws.amazon.com/blogs/machine-learning/automate-bulk-image-editing-with-crop-photo-and-amazon-rekognition/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Revolutionizing business processes with Amazon Bedrock and Appian’s generative AI skills</title>
      <link>https://www.dotnetramblings.com/post/10_02_2025/10_02_2025_5/</link>
      <pubDate>Mon, 10 Feb 2025 18:37:01 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/10_02_2025/10_02_2025_5/</guid>
      <description>
        
          
            AWS and Appian’s collaboration marks a significant advancement in business process automation. By using the power of Amazon Bedrock and Anthropic’s Claude models, Appian empowers enterprises to optimize and automate processes for greater efficiency and effectiveness. This blog post will cover how Appian AI skills build automation into organizations’ mission-critical processes to improve operational excellence, reduce costs, and build scalable solutions.
Link to article: https://aws.amazon.com/blogs/machine-learning/revolutionizing-business-processes-with-amazon-bedrock-and-appians-generative-ai-skills/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Governing the ML lifecycle at scale, Part 4: Scaling MLOps with security and governance controls</title>
      <link>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_2/</link>
      <pubDate>Fri, 07 Feb 2025 20:25:13 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_2/</guid>
      <description>
        
          
            This post provides detailed steps for setting up the key components of a multi-account ML platform. This includes configuring the ML Shared Services Account, which manages the central templates, model registry, and deployment pipelines; sharing the ML Admin and SageMaker Projects Portfolios from the central Service Catalog; and setting up the individual ML Development Accounts where data scientists can build and train models.
Link to article: https://aws.amazon.com/blogs/machine-learning/governing-the-ml-lifecycle-at-scale-part-4-scaling-mlops-with-security-and-governance-controls/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Accelerate your Amazon Q implementation: starter kits for SMBs</title>
      <link>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_3/</link>
      <pubDate>Fri, 07 Feb 2025 17:29:14 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_3/</guid>
      <description>
        
          
            Starter kits are complete, deployable solutions that address common, repeatable business problems. They deploy the services that make up a solution according to best practices, helping you optimize costs and become familiar with these kinds of architectural patterns without a large investment in training. In this post, we showcase a starter kit for Amazon Q Business. If you have a repository of documents that you need to turn into a knowledge base quickly, or simply want to test out the capabilities of Amazon Q Business without a large investment of time at the console, then this solution is for you.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Building the future of construction analytics: CONXAI’s AI inference on Amazon EKS</title>
      <link>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_4/</link>
      <pubDate>Fri, 07 Feb 2025 17:21:34 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_4/</guid>
      <description>
        
          
            CONXAI Technology GmbH is pioneering the development of an advanced AI platform for the Architecture, Engineering, and Construction (AEC) industry. In this post, we dive deep into how CONXAI hosts the state-of-the-art OneFormer segmentation model on AWS using Amazon Simple Storage Service (Amazon S3), Amazon Elastic Kubernetes Service (Amazon EKS), KServe, and NVIDIA Triton.
Link to article: https://aws.amazon.com/blogs/machine-learning/building-the-future-of-construction-analytics-conxais-ai-inference-on-amazon-eks/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>How Untold Studios empowers artists with an AI assistant built on Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_5/</link>
      <pubDate>Fri, 07 Feb 2025 17:06:09 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_5/</guid>
      <description>
        
          
            Untold Studios is a tech-driven, leading creative studio specializing in high-end visual effects and animation. This post details how we used Amazon Bedrock to create an AI assistant (Untold Assistant), providing artists with a straightforward way to access our internal resources through a natural language interface integrated directly into their existing Slack workflow.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-untold-studios-empowers-artists-with-an-ai-assistant-built-on-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Protect your DeepSeek model deployments with Amazon Bedrock Guardrails</title>
      <link>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_12/</link>
      <pubDate>Fri, 07 Feb 2025 02:29:52 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/07_02_2025/07_02_2025_12/</guid>
      <description>
        
          
            This blog post provides a comprehensive guide to implementing robust safety protections for DeepSeek-R1 and other open weight models using Amazon Bedrock Guardrails. By following this guide, you&#39;ll learn how to use the advanced capabilities of DeepSeek models while maintaining strong security controls and promoting ethical AI practices.
Link to article: https://aws.amazon.com/blogs/machine-learning/protect-your-deepseek-model-deployments-with-amazon-bedrock-guardrails/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Fine-tune and host SDXL models cost-effectively with AWS Inferentia2</title>
      <link>https://www.dotnetramblings.com/post/06_02_2025/06_02_2025_3/</link>
      <pubDate>Thu, 06 Feb 2025 18:07:55 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/06_02_2025/06_02_2025_3/</guid>
      <description>
        
          
            As technology continues to evolve, newer models are emerging, offering higher quality, increased flexibility, and faster image generation capabilities. One such groundbreaking model is Stable Diffusion XL (SDXL), released by StabilityAI, advancing the text-to-image generative AI technology to unprecedented heights. In this post, we demonstrate how to efficiently fine-tune the SDXL model using SageMaker Studio. We show how to then prepare the fine-tuned model to run on AWS Inferentia2 powered Amazon EC2 Inf2 instances, unlocking superior price performance for your inference workloads.
          
          
        
      </description>
    </item>
    
    <item>
      <title>How Aetion is using generative AI and Amazon Bedrock to translate scientific intent to results</title>
      <link>https://www.dotnetramblings.com/post/06_02_2025/06_02_2025_5/</link>
      <pubDate>Thu, 06 Feb 2025 17:55:49 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/06_02_2025/06_02_2025_5/</guid>
      <description>
        
          
            Aetion is a leading provider of decision-grade real-world evidence software to biopharma, payors, and regulatory agencies. In this post, we review how Aetion is using Amazon Bedrock to help streamline the analytical process toward producing decision-grade real-world evidence and enable users without data science expertise to interact with complex real-world datasets.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-aetion-is-using-generative-ai-and-amazon-bedrock-to-translate-scientific-intent-to-results/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Trellix lowers cost, increases speed, and adds delivery flexibility with cost-effective and performant Amazon Nova Micro and Amazon Nova Lite models</title>
      <link>https://www.dotnetramblings.com/post/05_02_2025/05_02_2025_0/</link>
      <pubDate>Wed, 05 Feb 2025 22:53:33 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/05_02_2025/05_02_2025_0/</guid>
      <description>
        
          
            This post discusses the adoption and evaluation of Amazon Nova foundation models by Trellix, a leading company delivering cybersecurity’s broadest AI-powered platform to over 53,000 customers worldwide.
Link to article: https://aws.amazon.com/blogs/machine-learning/trellix-lowers-cost-increases-speed-and-adds-delivery-flexibility-with-cost-effective-and-performant-amazon-nova-micro-and-amazon-nova-lite-models/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>OfferUp improved local results by 54% and relevance recall by 27% with multimodal search on Amazon Bedrock and Amazon OpenSearch Service</title>
      <link>https://www.dotnetramblings.com/post/05_02_2025/05_02_2025_8/</link>
      <pubDate>Wed, 05 Feb 2025 19:06:55 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/05_02_2025/05_02_2025_8/</guid>
      <description>
        
          
            In this post, we demonstrate how OfferUp transformed its foundational search architecture using Amazon Titan Multimodal Embeddings and OpenSearch Service, significantly increasing user engagement, improving search quality and offering users the ability to search with both text and images. OfferUp selected Amazon Titan Multimodal Embeddings and Amazon OpenSearch Service for their fully managed capabilities, enabling the development of a robust multimodal search solution with high accuracy and a faster time to market for search and recommendation use cases.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Enhancing LLM Capabilities with NeMo Guardrails on Amazon SageMaker JumpStart</title>
      <link>https://www.dotnetramblings.com/post/05_02_2025/05_02_2025_11/</link>
      <pubDate>Wed, 05 Feb 2025 17:50:14 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/05_02_2025/05_02_2025_11/</guid>
      <description>
        
          
            Integrating NeMo Guardrails with Large Language Models (LLMs) is a powerful step forward in deploying AI in customer-facing applications. The example of AnyCompany Pet Supplies illustrates how these technologies can enhance customer interactions while handling refusal and guiding the conversation toward the implemented outcomes. This journey towards ethical AI deployment is crucial for building sustainable, trust-based relationships with customers and shaping a future where technology aligns seamlessly with human values.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build a multi-interface AI assistant using Amazon Q and Slack with Amazon CloudFront clickable references from an Amazon S3 bucket</title>
      <link>https://www.dotnetramblings.com/post/05_02_2025/05_02_2025_12/</link>
      <pubDate>Wed, 05 Feb 2025 16:56:56 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/05_02_2025/05_02_2025_12/</guid>
      <description>
        
          
            There is consistent customer feedback that AI assistants are the most useful when users can interface with them within the productivity tools they already use on a daily basis, to avoid switching applications and context. Web applications like Amazon Q Business and Slack have become essential environments for modern AI assistant deployment. This post explores how diverse interfaces enhance user interaction, improve accessibility, and cater to varying preferences.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
