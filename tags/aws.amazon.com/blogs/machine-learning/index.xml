<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Tue, 25 Feb 2025 16:53:11 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How IDIADA optimized its intelligent chatbot with Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/25_02_2025/25_02_2025_3/</link>
      <pubDate>Tue, 25 Feb 2025 16:53:11 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/25_02_2025/25_02_2025_3/</guid>
      <description>
        
          
            In 2021, Applus+ IDIADA, a global partner to the automotive industry with over 30 years of experience supporting customers in product development activities through design, engineering, testing, and homologation services, established the Digital Solutions department. In this post, we showcase the research process undertaken to develop a classifier for human interactions in this AI-based environment using Amazon Bedrock.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-idiada-optimized-its-intelligent-chatbot-with-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Accelerate IaC troubleshooting with Amazon Bedrock Agents</title>
      <link>https://www.dotnetramblings.com/post/25_02_2025/25_02_2025_4/</link>
      <pubDate>Tue, 25 Feb 2025 16:19:03 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/25_02_2025/25_02_2025_4/</guid>
      <description>
        
          
            This post demonstrates how Amazon Bedrock Agents, combined with action groups and generative AI models, streamlines and accelerates the resolution of Terraform errors while maintaining compliance with environment security and operational guidelines.
Link to article: https://aws.amazon.com/blogs/machine-learning/accelerate-iac-troubleshooting-with-amazon-bedrock-agents/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Derive generative AI powered insights from Alation Cloud Services using Amazon Q Business Custom Connector</title>
      <link>https://www.dotnetramblings.com/post/25_02_2025/25_02_2025_5/</link>
      <pubDate>Tue, 25 Feb 2025 16:16:25 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/25_02_2025/25_02_2025_5/</guid>
      <description>
        
          
            In this post, we showcase a sample of how Alation’s business policies can be integrated with an Amazon Q Business application using a custom data source connector.
Link to article: https://aws.amazon.com/blogs/machine-learning/derive-generative-ai-powered-insights-from-alation-cloud-services-using-amazon-q-business-custom-connector/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Mistral-Small-24B-Instruct-2501 is now available on SageMaker Jumpstart and Amazon Bedrock Marketplace</title>
      <link>https://www.dotnetramblings.com/post/24_02_2025/24_02_2025_1/</link>
      <pubDate>Mon, 24 Feb 2025 21:02:10 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/24_02_2025/24_02_2025_1/</guid>
      <description>
        
          
            We’re excited to announce that Mistral-Small-24B-Instruct-2501—a twenty-four billion parameter large language model (LLM) from Mistral AI that’s optimized for low latency text generation tasks—is available for customers through Amazon SageMaker JumpStart and Amazon Bedrock Marketplace. In this post, we walk through how to discover, deploy, and use Mistral-Small-24B-Instruct-2501.
Link to article: https://aws.amazon.com/blogs/machine-learning/mistral-small-24b-instruct-2501-is-now-available-on-sagemaker-jumpstart-and-amazon-bedrock-marketplace/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>How Rocket Companies modernized their data science solution on AWS</title>
      <link>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_0/</link>
      <pubDate>Fri, 21 Feb 2025 18:45:46 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_0/</guid>
      <description>
        
          
            In this post, we share how we modernized Rocket Companies’ data science solution on AWS to increase the speed to delivery from eight weeks to under one hour, improve operational stability and support by reducing incident tickets by over 99% in 18 months, power 10 million automated data science and AI decisions made daily, and provide a seamless data science development experience.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-rocket-companies-modernized-their-data-science-solution-on-aws/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>AWS and DXC collaborate to deliver customizable, near real-time voice-to-voice translation capabilities for Amazon Connect</title>
      <link>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_1/</link>
      <pubDate>Fri, 21 Feb 2025 17:08:18 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_1/</guid>
      <description>
        
          
            In this post, we discuss how AWS and DXC used Amazon Connect and other AWS AI services to deliver near real-time V2V translation capabilities.
Link to article: https://aws.amazon.com/blogs/machine-learning/aws-and-dxc-collaborate-to-deliver-customizable-near-real-time-voice-to-voice-translation-capabilities-for-amazon-connect/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Orchestrate an intelligent document processing workflow using tools in Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_2/</link>
      <pubDate>Fri, 21 Feb 2025 16:44:25 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_2/</guid>
      <description>
        
          
            This intelligent document processing solution uses Amazon Bedrock FMs to orchestrate a sophisticated workflow for handling multi-page healthcare documents with mixed content types. The solution uses the FM’s tool use capabilities, accessed through the Amazon Bedrock Converse API. This enables the FMs to not just process text, but to actively engage with various external tools and APIs to perform complex document analysis tasks.
Link to article: https://aws.amazon.com/blogs/machine-learning/orchestrate-an-intelligent-document-processing-workflow-using-tools-in-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Reducing hallucinations in LLM agents with a verified semantic cache using Amazon Bedrock Knowledge Bases</title>
      <link>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_3/</link>
      <pubDate>Fri, 21 Feb 2025 16:36:30 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_3/</guid>
      <description>
        
          
            This post introduces a solution to reduce hallucinations in Large Language Models (LLMs) by implementing a verified semantic cache using Amazon Bedrock Knowledge Bases, which checks if user questions match curated and verified responses before generating new answers. The solution combines the flexibility of LLMs with reliable, verified answers to improve response accuracy, reduce latency, and lower costs while preventing potential misinformation in critical domains such as healthcare, finance, and legal services.
          
          
        
      </description>
    </item>
    
    <item>
      <title>LLM continuous self-instruct fine-tuning framework powered by a compound AI system on Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_4/</link>
      <pubDate>Fri, 21 Feb 2025 16:27:06 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_4/</guid>
      <description>
        
          
            In this post, we present the continuous self-instruct fine-tuning framework as a compound AI system implemented by the DSPy framework. The framework first generates a synthetic dataset from the domain knowledge base and documents for self-instruction, then drives model fine-tuning through SFT, and introduces the human-in-the-loop workflow to collect human and AI feedback to the model response, which is used to further improve the model performance by aligning human preference through reinforcement learning (RLHF/RLAIF).
          
          
        
      </description>
    </item>
    
    <item>
      <title>Maximize your file server data’s potential by using Amazon Q Business on Amazon FSx for Windows</title>
      <link>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_6/</link>
      <pubDate>Fri, 21 Feb 2025 16:17:51 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/21_02_2025/21_02_2025_6/</guid>
      <description>
        
          
            In this post, we show you how to connect Amazon Q, a generative AI-powered assistant, to Amazon FSx for Windows File Server to securely analyze, query, and extract insights from your file system data.
Link to article: https://aws.amazon.com/blogs/machine-learning/maximize-your-file-server-datas-potential-by-using-amazon-q-business-on-amazon-fsx-for-windows/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Generate synthetic counterparty (CR) risk data with generative AI using Amazon Bedrock LLMs and RAG</title>
      <link>https://www.dotnetramblings.com/post/20_02_2025/20_02_2025_3/</link>
      <pubDate>Thu, 20 Feb 2025 17:24:25 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/20_02_2025/20_02_2025_3/</guid>
      <description>
        
          
            In this post, we explore how you can use LLMs with advanced Retrieval Augmented Generation (RAG) to generate high-quality synthetic data for a finance domain use case. You can use the same technique for synthetic data for other business domain use cases as well. For this post, we demonstrate how to generate counterparty risk (CR) data, which would be beneficial for over-the-counter (OTC) derivatives that are traded directly between two parties, without going through a formal exchange.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Turbocharging premium audit capabilities with the power of generative AI: Verisk’s journey toward a sophisticated conversational chat platform to enhance customer support</title>
      <link>https://www.dotnetramblings.com/post/20_02_2025/20_02_2025_5/</link>
      <pubDate>Thu, 20 Feb 2025 17:13:17 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/20_02_2025/20_02_2025_5/</guid>
      <description>
        
          
            Verisk’s Premium Audit Advisory Service is the leading source of technical information and training for premium auditors and underwriters. In this post, we describe the development of the customer support process in PAAS, incorporating generative AI, the data, the architecture, and the evaluation of the results. Conversational AI assistants are rapidly transforming customer and employee support.
Link to article: https://aws.amazon.com/blogs/machine-learning/turbocharging-premium-audit-capabilities-with-the-power-of-generative-ai-verisks-journey-toward-a-sophisticated-conversational-chat-platform-to-enhance-customer-support/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build verifiable explainability into financial services workflows with Automated Reasoning checks for Amazon Bedrock Guardrails</title>
      <link>https://www.dotnetramblings.com/post/19_02_2025/19_02_2025_5/</link>
      <pubDate>Wed, 19 Feb 2025 17:30:22 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/19_02_2025/19_02_2025_5/</guid>
      <description>
        
          
            In this post, we explore how Automated Reasoning checks work through various common FSI scenarios such as insurance legal triaging, underwriting rules validation, and claims processing.
Link to article: https://aws.amazon.com/blogs/machine-learning/build-verifiable-explainability-into-financial-services-workflows-with-automated-reasoning-checks-for-amazon-bedrock-guardrails/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Best practices for Amazon SageMaker HyperPod task governance</title>
      <link>https://www.dotnetramblings.com/post/19_02_2025/19_02_2025_6/</link>
      <pubDate>Wed, 19 Feb 2025 17:28:04 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/19_02_2025/19_02_2025_6/</guid>
      <description>
        
          
            In this post, we provide best practices to maximize the value of SageMaker HyperPod task governance and make the administration and data science experiences seamless. We also discuss common governance scenarios when administering and running generative AI development tasks.
Link to article: https://aws.amazon.com/blogs/machine-learning/best-practices-for-amazon-sagemaker-hyperpod-task-governance/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
