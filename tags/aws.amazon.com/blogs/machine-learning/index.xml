<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Mon, 09 Dec 2024 17:55:22 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Accelerating ML experimentation with enhanced security: AWS PrivateLink support for Amazon SageMaker with MLflow</title>
      <link>https://www.dotnetramblings.com/post/09_12_2024/09_12_2024_2/</link>
      <pubDate>Mon, 09 Dec 2024 17:55:22 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/09_12_2024/09_12_2024_2/</guid>
      <description>
        
          
            With access to a wide range of generative AI foundation models (FM) and the ability to build and train their own machine learning (ML) models in Amazon SageMaker, users want a seamless and secure way to experiment with and select the models that deliver the most value for their business. In the initial stages of an ML […]
Link to article: https://aws.amazon.com/blogs/machine-learning/accelerating-ml-experimentation-with-enhanced-security-aws-privatelink-support-for-amazon-sagemaker-with-mlflow/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Mistral-NeMo-Instruct-2407 and Mistral-NeMo-Base-2407 are now available on SageMaker JumpStart</title>
      <link>https://www.dotnetramblings.com/post/06_12_2024/06_12_2024_1/</link>
      <pubDate>Fri, 06 Dec 2024 18:32:16 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/06_12_2024/06_12_2024_1/</guid>
      <description>
        
          
            Today, we are excited to announce that Mistral-NeMo-Base-2407 and Mistral-NeMo-Instruct-2407 large language models from Mistral AI that excel at text generation, are available for customers through Amazon SageMaker JumpStart. In this post, we walk through how to discover, deploy and use the Mistral-NeMo-Instruct-2407 and Mistral-NeMo-Base-2407 models for a variety of real-world use cases.
Link to article: https://aws.amazon.com/blogs/machine-learning/mistral-nemo-instruct-2407-and-mistral-nemo-base-2407-are-now-available-on-sagemaker-jumpstart/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Advancing AI trust with new responsible AI tools, capabilities, and resources</title>
      <link>https://www.dotnetramblings.com/post/05_12_2024/05_12_2024_0/</link>
      <pubDate>Thu, 05 Dec 2024 23:05:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/05_12_2024/05_12_2024_0/</guid>
      <description>
        
          
            With trust as a cornerstone of AI adoption, we are excited to announce at AWS re:Invent 2024 new responsible AI tools, capabilities, and resources that enhance the safety, security, and transparency of our AI services and models and help support customers’ own responsible AI journeys.
Link to article: https://aws.amazon.com/blogs/machine-learning/advancing-ai-trust-with-new-responsible-ai-tools-capabilities-and-resources/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Deploy RAG applications on Amazon SageMaker JumpStart using FAISS</title>
      <link>https://www.dotnetramblings.com/post/05_12_2024/05_12_2024_1/</link>
      <pubDate>Thu, 05 Dec 2024 21:57:11 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/05_12_2024/05_12_2024_1/</guid>
      <description>
        
          
            In this post, we show how to build a RAG application on Amazon SageMaker JumpStart using Facebook AI Similarity Search (FAISS).
Link to article: https://aws.amazon.com/blogs/machine-learning/deploy-rag-applications-on-amazon-sagemaker-jumpstart-using-faiss/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Speed up your cluster procurement time with Amazon SageMaker HyperPod training plans</title>
      <link>https://www.dotnetramblings.com/post/05_12_2024/05_12_2024_2/</link>
      <pubDate>Thu, 05 Dec 2024 18:20:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/05_12_2024/05_12_2024_2/</guid>
      <description>
        
          
            In this post, we demonstrate how you can use Amazon SageMaker HyperPod training plans, to bring down your training cluster procurement wait time. We guide you through a step-by-step implementation on how you can use the (AWS CLI) or the AWS Management Console to find, review, and create optimal training plans for your specific compute and timeline needs. We further guide you through using the training plan to submit SageMaker training jobs or create SageMaker HyperPod clusters.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Amazon Bedrock Marketplace now includes NVIDIA models: Introducing NVIDIA Nemotron-4 NIM microservices</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_2/</link>
      <pubDate>Wed, 04 Dec 2024 19:28:56 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_2/</guid>
      <description>
        
          
            At AWS re:Invent 2024, we are excited to introduce Amazon Bedrock Marketplace. This a revolutionary new capability within Amazon Bedrock that serves as a centralized hub for discovering, testing, and implementing foundation models (FMs). In this post, we discuss the advantages and capabilities of Amazon Bedrock Marketplace and Nemotron models, and how to get started.
Link to article: https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-marketplace-now-includes-nvidia-models-introducing-nvidia-nemotron-4-nim-microservices/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Real value, real time: Production AI with Amazon SageMaker and Tecton</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_4/</link>
      <pubDate>Wed, 04 Dec 2024 19:01:36 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_4/</guid>
      <description>
        
          
            In this post, we discuss how Amazon SageMaker and Tecton work together to simplify the development and deployment of production-ready AI applications, particularly for real-time use cases like fraud detection. The integration enables faster time to value by abstracting away complex engineering tasks, allowing teams to focus on building features and use cases while providing a streamlined framework for both offline training and online serving of ML models.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Use Amazon Bedrock tooling with Amazon SageMaker JumpStart models</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_5/</link>
      <pubDate>Wed, 04 Dec 2024 18:42:03 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_5/</guid>
      <description>
        
          
            In this post, we explore how to deploy AI models from SageMaker JumpStart and use them with Amazon Bedrock&#39;s powerful features. Users can combine SageMaker JumpStart&#39;s model hosting with Bedrock&#39;s security and monitoring tools. We demonstrate this using the Gemma 2 9B Instruct model as an example, showing how to deploy it and use Bedrock&#39;s advanced capabilities.
Link to article: https://aws.amazon.com/blogs/machine-learning/use-amazon-bedrock-tooling-with-amazon-sagemaker-jumpstart-models/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>A guide to Amazon Bedrock Model Distillation (preview)</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_6/</link>
      <pubDate>Wed, 04 Dec 2024 18:39:31 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_6/</guid>
      <description>
        
          
            This post introduces the workflow of Amazon Bedrock Model Distillation. We first introduce the general concept of model distillation in Amazon Bedrock, and then focus on the important steps in model distillation, including setting up permissions, selecting the models, providing input dataset, commencing the model distillation jobs, and conducting evaluation and deployment of the student models after model distillation.
Link to article: https://aws.amazon.com/blogs/machine-learning/a-guide-to-amazon-bedrock-model-distillation-preview/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build generative AI applications quickly with Amazon Bedrock IDE in Amazon SageMaker Unified Studio</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_7/</link>
      <pubDate>Wed, 04 Dec 2024 18:36:49 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_7/</guid>
      <description>
        
          
            In this post, we&#39;ll show how anyone in your company can use Amazon Bedrock IDE to quickly create a generative AI chat agent application that analyzes sales performance data. Through simple conversations, business teams can use the chat agent to extract valuable insights from both structured and unstructured data sources without writing code or managing complex data pipelines.
Link to article: https://aws.amazon.com/blogs/machine-learning/build-generative-ai-applications-quickly-with-amazon-bedrock-ide-in-amazon-sagemaker-unified-studio/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Scale ML workflows with Amazon SageMaker Studio and Amazon SageMaker HyperPod</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_8/</link>
      <pubDate>Wed, 04 Dec 2024 18:31:15 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_8/</guid>
      <description>
        
          
            The integration of Amazon SageMaker Studio and Amazon SageMaker HyperPod offers a streamlined solution that provides data scientists and ML engineers with a comprehensive environment that supports the entire ML lifecycle, from development to deployment at scale. In this post, we walk you through the process of scaling your ML workloads using SageMaker Studio and SageMaker HyperPod.
Link to article: https://aws.amazon.com/blogs/machine-learning/scale-ml-workflows-with-amazon-sagemaker-studio-and-amazon-sagemaker-hyperpod/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Introducing Amazon Kendra GenAI Index – Enhanced semantic search and retrieval capabilities</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_10/</link>
      <pubDate>Wed, 04 Dec 2024 17:28:17 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_10/</guid>
      <description>
        
          
            Amazon has introduced the Amazon Kendra GenAI Index, a new offering designed to enhance semantic search and retrieval capabilities for enterprise AI applications. This index is optimized for Retrieval Augmented Generation (RAG) and intelligent search, allowing businesses to build more effective digital assistants and search experiences.
Link to article: https://aws.amazon.com/blogs/machine-learning/introducing-amazon-kendra-genai-index-enhanced-semantic-search-and-retrieval-capabilities/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Building Generative AI and ML solutions faster with AI apps from AWS partners using Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_11/</link>
      <pubDate>Wed, 04 Dec 2024 17:03:49 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_12_2024/04_12_2024_11/</guid>
      <description>
        
          
            Today, we’re excited to announce that AI apps from AWS Partners are now available in SageMaker. You can now find, deploy, and use these AI apps privately and securely, all without leaving SageMaker AI, so you can develop performant AI models faster.
Link to article: https://aws.amazon.com/blogs/machine-learning/building-generative-ai-and-ml-solutions-faster-with-ai-apps-from-aws-partners-using-amazon-sagemaker/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Query structured data from Amazon Q Business using Amazon QuickSight integration</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_2/</link>
      <pubDate>Tue, 03 Dec 2024 19:36:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_2/</guid>
      <description>
        
          
            In this post, we show how Amazon Q Business integrates with QuickSight to enable users to query both structured and unstructured data in a unified way. The integration allows users to connect to over 20 structured data sources like Amazon Redshift and PostgreSQL, while getting real-time answers with visualizations. Amazon Q Business combines information from structured sources through QuickSight with unstructured content to provide comprehensive answers to user queries.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Elevate customer experience by using the Amazon Q Business custom plugin for New Relic AI</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_3/</link>
      <pubDate>Tue, 03 Dec 2024 18:57:22 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_3/</guid>
      <description>
        
          
            The New Relic AI custom plugin for Amazon Q Business creates a unified solution that combines New Relic AI’s observability insights and recommendations and Amazon Q Business’s Retrieval Augmented Generation (RAG) capabilities, in and a natural language interface for east of use. This post explores the use case, how this custom plugin works, how it can be enabled, and how it can help elevate customers’ digital experiences.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Amazon SageMaker launches the updated inference optimization toolkit for generative AI</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_4/</link>
      <pubDate>Tue, 03 Dec 2024 18:55:05 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_4/</guid>
      <description>
        
          
            Today, Amazon SageMaker is excited to announce updates to the inference optimization toolkit, providing new functionality and enhancements to help you optimize generative AI models even faster.In this post, we discuss these new features of the toolkit in more detail.
Link to article: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-launches-the-updated-inference-optimization-toolkit-for-generative-ai/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Syngenta develops a generative AI assistant to support sales representatives using Amazon Bedrock Agents</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_5/</link>
      <pubDate>Tue, 03 Dec 2024 18:52:47 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_5/</guid>
      <description>
        
          
            In this post, we explore how Syngenta collaborated with AWS to develop Cropwise AI, a generative AI assistant powered by Amazon Bedrock Agents that helps sales representatives make better seed product recommendations to farmers across North America. The solution transforms the seed selection process by simplifying complex data into natural conversations, providing quick access to detailed seed product information, and enabling personalized recommendations at scale through a mobile app interface.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Speed up your AI inference workloads with new NVIDIA-powered capabilities in Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_14/</link>
      <pubDate>Tue, 03 Dec 2024 00:51:21 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_14/</guid>
      <description>
        
          
            At re:Invent 2024, we are excited to announce new capabilities to speed up your AI inference workloads with NVIDIA accelerated computing and software offerings on Amazon SageMaker. In this post, we will explore how you can use these new capabilities to enhance your AI inference on Amazon SageMaker. We&#39;ll walk through the process of deploying NVIDIA NIM microservices from AWS Marketplace for SageMaker Inference. We&#39;ll then dive into NVIDIA’s model offerings on SageMaker JumpStart, showcasing how to access and deploy the Nemotron-4 model directly in the JumpStart interface.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Unlock cost savings with the new scale down to zero feature in SageMaker Inference</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_15/</link>
      <pubDate>Tue, 03 Dec 2024 00:51:14 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_15/</guid>
      <description>
        
          
            Today at AWS re:Invent 2024, we are excited to announce a new feature for Amazon SageMaker inference endpoints: the ability to scale SageMaker inference endpoints to zero instances. This long-awaited capability is a game changer for our customers using the power of AI and machine learning (ML) inference in the cloud.
Link to article: https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Supercharge your auto scaling for generative AI inference – Introducing Container Caching in SageMaker Inference</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_16/</link>
      <pubDate>Tue, 03 Dec 2024 00:51:06 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_16/</guid>
      <description>
        
          
            Today at AWS re:Invent 2024, we are excited to announce the new Container Caching capability in Amazon SageMaker, which significantly reduces the time required to scale generative AI models for inference. This innovation allows you to scale your models faster, observing up to 56% reduction in latency when scaling a new model copy and up to 30% when adding a model copy on a new instance. In this post, we explore the new Container Caching feature for SageMaker inference, addressing the challenges of deploying and scaling large language models (LLMs).
          
          
        
      </description>
    </item>
    
    <item>
      <title>Introducing Fast Model Loader in SageMaker Inference: Accelerate autoscaling for your Large Language Models (LLMs) – part 1</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_17/</link>
      <pubDate>Tue, 03 Dec 2024 00:50:57 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_17/</guid>
      <description>
        
          
            Today at AWS re:Invent 2024, we are excited to announce a new capability in Amazon SageMaker Inference that significantly reduces the time required to deploy and scale LLMs for inference using LMI: Fast Model Loader. In this post, we delve into the technical details of Fast Model Loader, explore its integration with existing SageMaker workflows, discuss how you can get started with this powerful new feature, and share customer success stories.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Introducing Fast Model Loader in SageMaker Inference: Accelerate autoscaling for your Large Language Models (LLMs) – Part 2</title>
      <link>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_18/</link>
      <pubDate>Tue, 03 Dec 2024 00:50:48 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_12_2024/03_12_2024_18/</guid>
      <description>
        
          
            In this post, we provide a detailed, hands-on guide to implementing Fast Model Loader in your LLM deployments. We explore two approaches: using the SageMaker Python SDK for programmatic implementation, and using the Amazon SageMaker Studio UI for a more visual, interactive experience. Whether you’re a developer who prefers working with code or someone who favors a graphical interface, you’ll learn how to take advantage of this powerful feature to accelerate your LLM deployments.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
