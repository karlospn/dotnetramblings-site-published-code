<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/</link>
    <description>Recent content in Aws.amazon.com/Blogs/Machine-Learning on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Sun, 01 Dec 2024 22:29:00 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/aws.amazon.com/blogs/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cohere Rerank 3.5 is now available in Amazon Bedrock through Rerank API</title>
      <link>https://www.dotnetramblings.com/post/01_12_2024/01_12_2024_0/</link>
      <pubDate>Sun, 01 Dec 2024 22:29:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/01_12_2024/01_12_2024_0/</guid>
      <description>
        
          
            We are excited to announce the availability of Cohere’s advanced reranking model Rerank 3.5 through our new Rerank API in Amazon Bedrock. This powerful reranking model enables AWS customers to significantly improve their search relevance and content ranking capabilities. In this post, we discuss the need for Reranking, the capabilities of Cohere’s Rerank 3.5, and how to get started using it on Amazon Bedrock.
Link to article: https://aws.amazon.com/blogs/machine-learning/cohere-rerank-3-5-is-now-available-in-amazon-bedrock-through-rerank-api/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>AWS DeepRacer: How to master physical racing?</title>
      <link>https://www.dotnetramblings.com/post/01_12_2024/01_12_2024_1/</link>
      <pubDate>Sun, 01 Dec 2024 22:20:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/01_12_2024/01_12_2024_1/</guid>
      <description>
        
          
            In this blog post, I will look at what makes physical AWS DeepRacer racing—a real car on a real track—different to racing in the virtual world—a model in a simulated 3D environment. I will cover the basics, the differences in virtual compared to physical, and what steps I have taken to get a deeper understanding of the challenge.
Link to article: https://aws.amazon.com/blogs/machine-learning/aws-deepracer-how-to-master-physical-racing/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Easily deploy and manage hundreds of LoRA adapters with SageMaker efficient multi-adapter inference</title>
      <link>https://www.dotnetramblings.com/post/29_11_2024/29_11_2024_0/</link>
      <pubDate>Fri, 29 Nov 2024 20:44:39 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_11_2024/29_11_2024_0/</guid>
      <description>
        
          
            The new efficient multi-adapter inference feature of Amazon SageMaker unlocks exciting possibilities for customers using fine-tuned models. This capability integrates with SageMaker inference components to allow you to deploy and manage hundreds of fine-tuned Low-Rank Adaptation (LoRA) adapters through SageMaker APIs. In this post, we show how to use the new efficient multi-adapter inference feature in SageMaker.
Link to article: https://aws.amazon.com/blogs/machine-learning/easily-deploy-and-manage-hundreds-of-lora-adapters-with-sagemaker-efficient-multi-adapter-inference/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Improve the performance of your Generative AI applications with Prompt Optimization on Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/29_11_2024/29_11_2024_1/</link>
      <pubDate>Fri, 29 Nov 2024 19:49:30 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/29_11_2024/29_11_2024_1/</guid>
      <description>
        
          
            Today, we are excited to announce the availability of Prompt Optimization on Amazon Bedrock. With this capability, you can now optimize your prompts for several use cases with a single API call or a click of a button on the Amazon Bedrock console. In this post, we discuss how you can get started with this new feature using an example use case in addition to discussing some performance benchmarks.
Link to article: https://aws.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Embodied AI Chess with Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_0/</link>
      <pubDate>Wed, 27 Nov 2024 23:03:18 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_0/</guid>
      <description>
        
          
            In this post, we demonstrate Embodied AI Chess with Amazon Bedrock, bringing a new dimension to traditional chess through generative AI capabilities. Our setup features a smart chess board that can detect moves in real time, paired with two robotic arms executing those moves. Each arm is controlled by different FMs—base or custom. This physical implementation allows you to observe and experiment with how different generative AI models approach complex gaming strategies in real-world chess matches.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Efficiently train models with large sequence lengths using Amazon SageMaker model parallel</title>
      <link>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_4/</link>
      <pubDate>Wed, 27 Nov 2024 20:39:54 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_4/</guid>
      <description>
        
          
            In this post, we demonstrate how the Amazon SageMaker model parallel library (SMP) addresses this need through support for new features such as 8-bit floating point (FP8) mixed-precision training for accelerated training performance and context parallelism for processing large input sequence lengths, expanding the list of its existing features.
Link to article: https://aws.amazon.com/blogs/machine-learning/efficiently-train-models-with-large-sequence-lengths-using-amazon-sagemaker-model-parallel/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Getting started with Amazon Bedrock Agents custom orchestrator</title>
      <link>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_5/</link>
      <pubDate>Wed, 27 Nov 2024 20:32:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_5/</guid>
      <description>
        
          
            In this post, we explore how Amazon Bedrock Agents simplify the orchestration of generative AI workflows, particularly with the introduction of the custom orchestrator feature. You can use the custom orchestrator to fine-tune and optimize agentic workflows that align more closely with specific business and operational needs. We outline the feature’s key benefits, including full control over orchestration, real-time adjustments, and reusability, followed by a breakdown of how it manages state transitions and contract-based interactions between Amazon Bedrock Agents and AWS Lambda.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Use Amazon Bedrock Agents for code scanning, optimization, and remediation</title>
      <link>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_6/</link>
      <pubDate>Wed, 27 Nov 2024 19:37:47 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_6/</guid>
      <description>
        
          
            For enterprises in the realm of cloud computing and software development, providing secure code repositories is essential. As sophisticated cybersecurity threats become more prevalent, organizations must adopt proactive measures to protect their assets. Amazon Bedrock offers a powerful solution by automating the process of scanning repositories for vulnerabilities and remediating them. This post explores how you can use Amazon Bedrock to enhance the security of your repositories and maintain compliance with organizational and regulatory standards.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Create a generative AI assistant with Slack and Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_13/</link>
      <pubDate>Wed, 27 Nov 2024 14:57:05 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/27_11_2024/27_11_2024_13/</guid>
      <description>
        
          
            Seamless integration of customer experience, collaboration tools, and relevant data is the foundation for delivering knowledge-based productivity gains. In this post, we show you how to integrate the popular Slack messaging service with AWS generative AI services to build a natural language assistant where business users can ask questions of an unstructured dataset.
Link to article: https://aws.amazon.com/blogs/machine-learning/create-a-generative-ai-assistant-with-slack-and-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Unleash your Salesforce data using the Amazon Q Salesforce Online connector</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_0/</link>
      <pubDate>Tue, 26 Nov 2024 23:09:31 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_0/</guid>
      <description>
        
          
            In this post, we walk you through configuring and setting up the Amazon Q Salesforce Online connector. Thousands of companies worldwide use Salesforce to manage their sales, marketing, customer service, and other business operations. The Salesforce cloud-based platform centralizes customer information and interactions across the organization, providing sales reps, marketers, and support agents with a unified 360-degree view of each customer. With Salesforce at the heart of their business, companies accumulate vast amounts of customer data within the platform over time.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Reducing hallucinations in large language models with custom intervention using Amazon Bedrock Agents</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_1/</link>
      <pubDate>Tue, 26 Nov 2024 22:14:48 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_1/</guid>
      <description>
        
          
            This post demonstrates how to use Amazon Bedrock Agents, Amazon Knowledge Bases, and the RAGAS evaluation metrics to build a custom hallucination detector and remediate it by using human-in-the-loop. The agentic workflow can be extended to custom use cases through different hallucination remediation techniques and offers the flexibility to detect and mitigate hallucinations using custom actions.
Link to article: https://aws.amazon.com/blogs/machine-learning/reducing-hallucinations-in-large-language-models-with-custom-intervention-using-amazon-bedrock-agents/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Deploy Meta Llama 3.1-8B on AWS Inferentia using Amazon EKS and vLLM</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_2/</link>
      <pubDate>Tue, 26 Nov 2024 22:12:34 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_2/</guid>
      <description>
        
          
            In this post, we walk through the steps to deploy the Meta Llama 3.1-8B model on Inferentia 2 instances using Amazon EKS. This solution combines the exceptional performance and cost-effectiveness of Inferentia 2 chips with the robust and flexible landscape of Amazon EKS. Inferentia 2 chips deliver high throughput and low latency inference, ideal for LLMs.
Link to article: https://aws.amazon.com/blogs/machine-learning/deploy-meta-llama-3-1-8b-on-aws-inferentia-using-amazon-eks-and-vllm/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Serving LLMs using vLLM and Amazon EC2 instances with AWS AI chips</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_3/</link>
      <pubDate>Tue, 26 Nov 2024 22:07:52 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_3/</guid>
      <description>
        
          
            The use of large language models (LLMs) and generative AI has exploded over the last year. With the release of powerful publicly available foundation models, tools for training, fine tuning and hosting your own LLM have also become democratized. Using vLLM on AWS Trainium and Inferentia makes it possible to host LLMs for high performance […]
Link to article: https://aws.amazon.com/blogs/machine-learning/serving-llms-using-vllm-and-amazon-ec2-instances-with-aws-ai-chips/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Using LLMs to fortify cyber defenses: Sophos’s insight on strategies for using LLMs with Amazon Bedrock and Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_5/</link>
      <pubDate>Tue, 26 Nov 2024 18:52:46 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_5/</guid>
      <description>
        
          
            In this post, SophosAI shares insights in using and evaluating an out-of-the-box LLM for the enhancement of a security operations center’s (SOC) productivity using Amazon Bedrock and Amazon SageMaker. We use Anthropic’s Claude 3 Sonnet on Amazon Bedrock to illustrate the use cases.
Link to article: https://aws.amazon.com/blogs/machine-learning/using-llms-to-fortify-cyber-defenses-sophoss-insight-on-strategies-for-using-llms-with-amazon-bedrock-and-amazon-sagemaker/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Enhanced observability for AWS Trainium and AWS Inferentia with Datadog</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_7/</link>
      <pubDate>Tue, 26 Nov 2024 17:53:11 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_7/</guid>
      <description>
        
          
            This post walks you through Datadog’s new integration with AWS Neuron, which helps you monitor your AWS Trainium and AWS Inferentia instances by providing deep observability into resource utilization, model execution performance, latency, and real-time infrastructure health, enabling you to optimize machine learning (ML) workloads and achieve high-performance at scale.
Link to article: https://aws.amazon.com/blogs/machine-learning/enhanced-observability-for-aws-trainium-and-aws-inferentia-with-datadog/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Create a virtual stock technical analyst using Amazon Bedrock Agents</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_8/</link>
      <pubDate>Tue, 26 Nov 2024 17:19:09 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_8/</guid>
      <description>
        
          
            n this post, we create a virtual analyst that can answer natural language queries of stocks matching certain technical indicator criteria using Amazon Bedrock Agents.
Link to article: https://aws.amazon.com/blogs/machine-learning/create-a-virtual-stock-technical-analyst-using-amazon-bedrock-agents/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Apply Amazon SageMaker Studio lifecycle configurations using AWS CDK</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_9/</link>
      <pubDate>Tue, 26 Nov 2024 17:15:27 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_9/</guid>
      <description>
        
          
            This post serves as a step-by-step guide on how to set up lifecycle configurations for your Amazon SageMaker Studio domains. With lifecycle configurations, system administrators can apply automated controls to their SageMaker Studio domains and their users. We cover core concepts of SageMaker Studio and provide code examples of how to apply lifecycle configuration to […]
Link to article: https://aws.amazon.com/blogs/machine-learning/apply-amazon-sagemaker-studio-lifecycle-configurations-using-aws-cdk/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Build a read-through semantic cache with Amazon OpenSearch Serverless and Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_10/</link>
      <pubDate>Tue, 26 Nov 2024 17:05:50 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_10/</guid>
      <description>
        
          
            This post presents a strategy for optimizing LLM-based applications. Given the increasing need for efficient and cost-effective AI solutions, we present a serverless read-through caching blueprint that uses repeated data patterns. With this cache, developers can effectively save and access similar prompts, thereby enhancing their systems’ efficiency and response times.
Link to article: https://aws.amazon.com/blogs/machine-learning/build-a-read-through-semantic-cache-with-amazon-opensearch-serverless-and-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Rad AI reduces real-time inference latency by 50% using Amazon SageMaker</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_11/</link>
      <pubDate>Tue, 26 Nov 2024 16:59:23 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_11/</guid>
      <description>
        
          
            This post is co-written with Ken Kao and Hasan Ali Demirci from Rad AI. Rad AI has reshaped radiology reporting, developing solutions that streamline the most tedious and repetitive tasks, and saving radiologists’ time. Since 2018, using state-of-the-art proprietary and open source large language models (LLMs), our flagship product—Rad AI Impressions— has significantly reduced the […]
Link to article: https://aws.amazon.com/blogs/machine-learning/rad-ai-reduces-real-time-inference-latency-by-50-using-amazon-sagemaker/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Read graphs, diagrams, tables, and scanned pages using multimodal prompts in Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_12/</link>
      <pubDate>Tue, 26 Nov 2024 16:48:42 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_12/</guid>
      <description>
        
          
            In this post, we demonstrate how to use models on Amazon Bedrock to retrieve information from images, tables, and scanned documents. We provide the following examples: 1/ performing object classification and object detection tasks, 2/ reading and querying graphs, and 3/ reading flowcharts and architecture diagrams (such as an AWS architecture diagram) and converting it to text.
Link to article: https://aws.amazon.com/blogs/machine-learning/read-graphs-diagrams-tables-and-scanned-pages-using-multimodal-prompts-in-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>How Crexi achieved ML models deployment on AWS at scale and boosted efficiency</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_14/</link>
      <pubDate>Tue, 26 Nov 2024 16:35:15 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_14/</guid>
      <description>
        
          
            Commercial Real Estate Exchange, Inc. (Crexi), is a digital marketplace and platform designed to streamline commercial real estate transactions. In this post, we will review how Crexi achieved its business needs and developed a versatile and powerful framework for AI/ML pipeline creation and deployment. This customizable and scalable solution allows its ML models to be efficiently deployed and managed to meet diverse project requirements.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-crexi-achieved-ml-models-deployment-on-aws-at-scale-and-boosted-efficiency/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Deploy Meta Llama 3.1 models cost-effectively in Amazon SageMaker JumpStart with AWS Inferentia and AWS Trainium</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_21/</link>
      <pubDate>Tue, 26 Nov 2024 00:37:42 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_21/</guid>
      <description>
        
          
            We’re excited to announce the availability of Meta Llama 3.1 8B and 70B inference support on AWS Trainium and AWS Inferentia instances in Amazon SageMaker JumpStart. Trainium and Inferentia, enabled by the AWS Neuron software development kit (SDK), offer high performance and lower the cost of deploying Meta Llama 3.1 by up to 50%. In this post, we demonstrate how to deploy Meta Llama 3.1 on Trainium and Inferentia instances in SageMaker JumpStart.
          
          
        
      </description>
    </item>
    
    <item>
      <title>AWS achieves ISO/IEC 42001:2023 Artificial Intelligence Management System accredited certification</title>
      <link>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_22/</link>
      <pubDate>Tue, 26 Nov 2024 00:11:26 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/26_11_2024/26_11_2024_22/</guid>
      <description>
        
          
            Amazon Web Services (AWS) is excited to be the first major cloud service provider to announce ISO/IEC 42001 accredited certification for the following AI services: Amazon Bedrock, Amazon Q Business, Amazon Textract, and Amazon Transcribe. ISO/IEC 42001 is an international management system standard that outlines requirements and controls for organizations to promote the responsible development and use of AI systems.
Link to article: https://aws.amazon.com/blogs/machine-learning/aws-achieves-iso-iec-420012023-artificial-intelligence-management-system-accredited-certification/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>How 123RF saved over 90% of their translation costs by switching to Amazon Bedrock</title>
      <link>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_4/</link>
      <pubDate>Mon, 25 Nov 2024 16:01:36 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_4/</guid>
      <description>
        
          
            This post explores how 123RF used Amazon Bedrock, Anthropic’s Claude 3 Haiku, and a vector store to efficiently translate content metadata, significantly reduce costs, and improve their global content discovery capabilities.
Link to article: https://aws.amazon.com/blogs/machine-learning/how-123rf-saved-over-90-of-their-translation-costs-by-switching-to-amazon-bedrock/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Connect SharePoint Online to Amazon Q Business using OAuth 2.0 ROPC flow authentication</title>
      <link>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_5/</link>
      <pubDate>Mon, 25 Nov 2024 15:59:15 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_5/</guid>
      <description>
        
          
            In this post, we explore how to integrate Amazon Q Business with SharePoint Online using the OAuth 2.0 ROPC flow authentication method. We provide both manual and automated approaches using PowerShell scripts for configuring the required Azure AD settings. Additionally, we demonstrate how to enter those details along with your SharePoint authentication credentials into the Amazon Q console to finalize the secure connection.
Link to article: https://aws.amazon.com/blogs/machine-learning/connect-sharepoint-online-to-amazon-q-business-using-oauth-2-0-ropc-flow-authentication/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>John Snow Labs Medical LLMs are now available in Amazon SageMaker JumpStart</title>
      <link>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_6/</link>
      <pubDate>Mon, 25 Nov 2024 15:55:17 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/25_11_2024/25_11_2024_6/</guid>
      <description>
        
          
            Today, we are excited to announce that John Snow Labs’ Medical LLM – Small and Medical LLM – Medium large language models (LLMs) are now available on Amazon SageMaker Jumpstart. For medical doctors, this tool provides a rapid understanding of a patient’s medical journey, aiding in timely and informed decision-making from extensive documentation. This summarization capability not only boosts efficiency but also makes sure that no critical details are overlooked, thereby supporting optimal patient care and enhancing healthcare outcomes.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
