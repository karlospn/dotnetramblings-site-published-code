<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker.com on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/docker.com/</link>
    <description>Recent content in Docker.com on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Wed, 08 Oct 2025 13:00:00 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/docker.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From the Captain’s Chair: Pradumna Saraf</title>
      <link>https://www.dotnetramblings.com/post/08_10_2025/08_10_2025_4/</link>
      <pubDate>Wed, 08 Oct 2025 13:00:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/08_10_2025/08_10_2025_4/</guid>
      <description>
        
          
            Docker Captains are leaders from the developer community that are both experts in their field and are passionate about sharing their Docker knowledge with others. “From the Captain’s Chair” is a blog series where we get a closer look at one Captain to learn more about them and their experiences. Today, we are interviewing Pradumna...
Link to article: https://www.docker.com/blog/from-the-captains-chair-pradumna-saraf/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Unlocking Local AI on Any GPU: Docker Model Runner Now with Vulkan Support</title>
      <link>https://www.dotnetramblings.com/post/08_10_2025/08_10_2025_6/</link>
      <pubDate>Wed, 08 Oct 2025 11:50:05 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/08_10_2025/08_10_2025_6/</guid>
      <description>
        
          
            Running large language models (LLMs) on your local machine is one of the most exciting frontiers in AI development. At Docker, our goal is to make this process as simple and accessible as possible. That’s why we built Docker Model Runner, a tool to help you download and run LLMs with a single command. Until...
Link to article: https://www.docker.com/blog/docker-model-runner-vulkan-gpu-support/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Powered by Docker: How Open Source Genius Cut Entropy Debt with Docker MCP Toolkit and Claude Desktop</title>
      <link>https://www.dotnetramblings.com/post/07_10_2025/07_10_2025_8/</link>
      <pubDate>Tue, 07 Oct 2025 13:00:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/07_10_2025/07_10_2025_8/</guid>
      <description>
        
          
            This is part of the Powered by Docker series, where we feature use cases and success stories from Docker partners and practitioners. This story was contributed by Ryan Wanner. Ryan has more than fifteen years of experience as an entrepreneur and 3 years in AI space developing software and is the founder of Open Source...
Link to article: https://www.docker.com/blog/open-source-genius-cut-entropy-debt-docker-mcp-claude/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>IBM Granite 4.0 Models Now Available on Docker Hub</title>
      <link>https://www.dotnetramblings.com/post/06_10_2025/06_10_2025_0/</link>
      <pubDate>Mon, 06 Oct 2025 21:29:23 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/06_10_2025/06_10_2025_0/</guid>
      <description>
        
          
            Developers can now discover and run IBM’s latest open-source Granite 4.0 language models from the Docker Hub model catalog, and start building in minutes with Docker Model Runner. Granite 4.0 pairs strong, enterprise-ready performance with a lightweight footprint, so you can prototype locally and scale confidently. The Granite 4.0 family is designed for speed, flexibility,...
Link to article: https://www.docker.com/blog/ibm-granite-4-0-models-now-available-on-docker-hub/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Unlimited access to Docker Hardened Images: Because security should be affordable, always</title>
      <link>https://www.dotnetramblings.com/post/06_10_2025/06_10_2025_1/</link>
      <pubDate>Mon, 06 Oct 2025 21:19:21 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/06_10_2025/06_10_2025_1/</guid>
      <description>
        
          
            Every organization we speak with shares the same goal: to deliver software that is secure and free of CVEs. Near-zero CVEs is the ideal state. But achieving that ideal is harder than it sounds, because paradoxes exist at every step. Developers patch quickly, yet new CVEs appear faster than fixes can ship. Organizations standardize on...
Link to article: https://www.docker.com/blog/unlimited-access-to-docker-hardened-images-because-security-should-be-affordable-always/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Docker at AI Engineer Paris: Build and Secure AI Agents with Docker</title>
      <link>https://www.dotnetramblings.com/post/06_10_2025/06_10_2025_8/</link>
      <pubDate>Mon, 06 Oct 2025 13:00:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/06_10_2025/06_10_2025_8/</guid>
      <description>
        
          
            Last week, Docker was thrilled to be part of the inaugural AI Engineer Paris, a spectacular European debut that brought together an extraordinary lineup of speakers and companies. The conference, organized by the Koyeb team, made one thing clear: the days of simply sprinkling &#39;AI dust&#39; on applications are over. Meaningful results demand rigorous engineering,...
Link to article: https://www.docker.com/blog/ai-engineer-paris-build-secure-ai-agents/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Llama.cpp Gets an Upgrade: Resumable Model Downloads</title>
      <link>https://www.dotnetramblings.com/post/06_10_2025/06_10_2025_11/</link>
      <pubDate>Mon, 06 Oct 2025 11:55:38 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/06_10_2025/06_10_2025_11/</guid>
      <description>
        
          
            We&#39;ve all been there: you&#39;re 90% of the way through downloading a massive, multi-gigabyte GGUF model file for llama.cpp when your internet connection hiccups. The download fails, and the progress bar resets to zero. It&#39;s a frustrating experience that wastes time, bandwidth, and momentum. Well, the llama.cpp community has just shipped a fantastic quality-of-life improvement...
Link to article: https://www.docker.com/blog/llama-cpp-resumable-gguf-downloads/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>From Shell Scripts to Science Agents: How AI Agents Are Transforming Research Workflows</title>
      <link>https://www.dotnetramblings.com/post/02_10_2025/02_10_2025_9/</link>
      <pubDate>Thu, 02 Oct 2025 13:00:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/02_10_2025/02_10_2025_9/</guid>
      <description>
        
          
            It’s 2 AM in a lab somewhere. A researcher has three terminals open, a half-written Jupyter notebook on one screen, an Excel sheet filled with sample IDs on another, and a half-eaten snack next to shell commands. They’re juggling scripts to run a protein folding model, parsing CSVs from the last experiment, searching for literature,...
Link to article: https://www.docker.com/blog/ai-science-agents-research-workflows/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Fine-Tuning Local Models with Docker Offload and Unsloth</title>
      <link>https://www.dotnetramblings.com/post/02_10_2025/02_10_2025_10/</link>
      <pubDate>Thu, 02 Oct 2025 11:46:39 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/02_10_2025/02_10_2025_10/</guid>
      <description>
        
          
            I&#39;ve been experimenting with local models for a while now, and the progress in making them accessible has been exciting. Initial experiences are often fantastic, many models, like Gemma 3 270M, are lightweight enough to run on common hardware. This potential for broad deployment is a major draw. However, as I&#39;ve tried to build meaningful,...
Link to article: https://www.docker.com/blog/fine-tuning-models-with-offload-and-unsloth/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
