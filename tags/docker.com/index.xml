<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker.com on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/docker.com/</link>
    <description>Recent content in Docker.com on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Fri, 11 Apr 2025 00:10:20 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/docker.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>New Docker Extension for Visual Studio Code</title>
      <link>https://www.dotnetramblings.com/post/11_04_2025/11_04_2025_4/</link>
      <pubDate>Fri, 11 Apr 2025 00:10:20 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/11_04_2025/11_04_2025_4/</guid>
      <description>
        
          
            Speed up development with Docker DX extension with real-time feedback, smarter linting, and intuitive Bake/Compose file support in VS Code.
Link to article: https://www.docker.com/blog/docker-dx-extension-for-vs-code/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Run Gemma 3 with Docker Model Runner: Fully Local GenAI Developer Experience</title>
      <link>https://www.dotnetramblings.com/post/09_04_2025/09_04_2025_10/</link>
      <pubDate>Wed, 09 Apr 2025 13:01:39 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/09_04_2025/09_04_2025_10/</guid>
      <description>
        
          
            Explore how to run Gemma 3 models locally using Docker Model Runner, alongside a Comment Processing System as a practical case study.
Link to article: https://www.docker.com/blog/run-gemma-3-locally-with-docker-model-runner/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Introducing Docker Model Runner: A Better Way to Build and Run GenAI Models Locally</title>
      <link>https://www.dotnetramblings.com/post/09_04_2025/09_04_2025_11/</link>
      <pubDate>Wed, 09 Apr 2025 13:00:44 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/09_04_2025/09_04_2025_11/</guid>
      <description>
        
          
            Docker Model Runner is a faster, simpler way to run and test AI models locally, right from your existing workflow.
Link to article: https://www.docker.com/blog/introducing-docker-model-runner/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Run LLMs Locally with Docker: A Quickstart Guide to Model Runner</title>
      <link>https://www.dotnetramblings.com/post/04_04_2025/04_04_2025_0/</link>
      <pubDate>Fri, 04 Apr 2025 20:15:32 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_04_2025/04_04_2025_0/</guid>
      <description>
        
          
            AI is quickly becoming a core part of modern applications, but running large language models (LLMs) locally can still be a pain. Between picking the right model, navigating hardware quirks, and optimizing for performance, it’s easy to get stuck before you even start building. At the same time, more and more developers want the flexibility […]
Link to article: https://www.docker.com/blog/run-llms-locally/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
