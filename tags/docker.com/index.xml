<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker.com on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/docker.com/</link>
    <description>Recent content in Docker.com on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Wed, 09 Apr 2025 13:01:39 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/docker.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Run Gemma 3 with Docker Model Runner: Fully Local GenAI Developer Experience</title>
      <link>https://www.dotnetramblings.com/post/09_04_2025/09_04_2025_0/</link>
      <pubDate>Wed, 09 Apr 2025 13:01:39 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/09_04_2025/09_04_2025_0/</guid>
      <description>
        
          
            Explore how to run Gemma 3 models locally using Docker Model Runner, alongside a Comment Processing System as a practical case study.
Link to article: https://www.docker.com/blog/run-gemma-3-locally-with-docker-model-runner/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Introducing Docker Model Runner: A Better Way to Build and Run GenAI Models Locally</title>
      <link>https://www.dotnetramblings.com/post/09_04_2025/09_04_2025_1/</link>
      <pubDate>Wed, 09 Apr 2025 13:00:44 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/09_04_2025/09_04_2025_1/</guid>
      <description>
        
          
            Docker Model Runner is a faster, simpler way to run and test AI models locally, right from your existing workflow.
Link to article: https://www.docker.com/blog/introducing-docker-model-runner/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Run LLMs Locally with Docker: A Quickstart Guide to Model Runner</title>
      <link>https://www.dotnetramblings.com/post/04_04_2025/04_04_2025_0/</link>
      <pubDate>Fri, 04 Apr 2025 20:15:32 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/04_04_2025/04_04_2025_0/</guid>
      <description>
        
          
            AI is quickly becoming a core part of modern applications, but running large language models (LLMs) locally can still be a pain. Between picking the right model, navigating hardware quirks, and optimizing for performance, it’s easy to get stuck before you even start building. At the same time, more and more developers want the flexibility […]
Link to article: https://www.docker.com/blog/run-llms-locally/ 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Master Docker and VS Code: Supercharge Your Dev Workflow</title>
      <link>https://www.dotnetramblings.com/post/03_04_2025/03_04_2025_1/</link>
      <pubDate>Thu, 03 Apr 2025 20:54:57 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/03_04_2025/03_04_2025_1/</guid>
      <description>
        
          
            Get step-by-step instructions on how to pair VS Code and Docker to streamline your development processes.
Link to article: https://www.docker.com/blog/master-docker-vs-code-supercharge-your-dev-workflow/ 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
