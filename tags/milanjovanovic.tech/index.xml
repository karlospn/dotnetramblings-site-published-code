<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Milanjovanovic.tech on .NET Ramblings</title>
    <link>https://www.dotnetramblings.com/tags/milanjovanovic.tech/</link>
    <description>Recent content in Milanjovanovic.tech on .NET Ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>.NET Ramblings</copyright>
    <lastBuildDate>Sun, 12 Jan 2025 12:30:41 +0000</lastBuildDate><atom:link href="https://www.dotnetramblings.com/tags/milanjovanovic.tech/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Working with LLMs in .NET using Microsoft.Extensions.AI</title>
      <link>https://www.dotnetramblings.com/post/11_01_2025/11_01_2025_2/</link>
      <pubDate>Sat, 11 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.dotnetramblings.com/post/11_01_2025/11_01_2025_2/</guid>
      <description>
        
          
            Microsoft.Extensions.AI provides a unified interface for integrating LLMs into .NET applications, allowing developers to switch between providers like Ollama, Azure, or OpenAI without changing application code. Through practical examples of chat completion, article summarization, and smart categorization, this article demonstrates how to leverage the library&#39;s features while running LLMs locally using Ollama.
Link to article: https://www.milanjovanovic.tech/blog/working-with-llms-in-dotnet-using-microsoft-extensions-ai 
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
